% mri parameter estimation via regression with kernels (PERK)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{s,perk,intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In quantitative magnetic resonance imaging (QMRI),
one seeks to estimate latent parameter images 
from suitably informative data.
Since MR acquisitions are tunably sensitive 
to many physical processes
(\eg, relaxation \cite{bloch:1946:ni-paper}, 
diffusion \cite{torrey:56:bew},
and chemical exchange \cite{mcconnell:58:rrb}),
MRI parameter estimation is important
for many QMRI applications
(\eg, relaxometry \cite{bloembergen:1948:rei}, 
diffusion tensor imaging \cite{bihan:01:dti}, 
and multi-compartmental imaging \cite{mackay:94:ivv}). 
Motivated by widespread applications,
this chapter introduces a general method
for fast MRI parameter estimation.

% signal models nonlinear
% so parameter estimation requires nonconvex optimization
% previous chapter described likelihood models
% several works [cite] have had success with this
% however these work for simple problems like single t1/t2 estimation
% for larger problems, undesirable or even intractable
% staroswiecki:12:seo 	fit m0,t1,t2,adc from dess im per-voxel
% ma:13:mrf		 					fit m0,t1,t2,b0 from mrf im per-voxel
% mcgivney:14:scf 	 		fit m0,t1,t2,b0 from low-rank mrf im using low-rank dict per-voxel
% zhao:14:mbm						recon m0,t2 from sse data via sparsity-constrained recon
% zhao:15:amp 					low-rank plus sparse recon w varpro for t1,t2 mapping
% beneliezer:15:raa 		fit m0,t2,b1 from mse im per-voxel
% zhao:16:mlr						recon m0,t1,t2 maps from mrf data via admm/varpro
% nataraj:17:oms				fit m0,t1,t2 from spgr/dess per-voxel
% asslander::lra 				similar to zhao:16:mlr, but claims better-conditioned initialization?
% cauley:15:fgm 				cluster full dict elements; use dict means to fit m0,t1,t2,b0 per-voxel
% doneva:17:mcb					est subspace of mrf k-t data from fully-sampled k-space center
% 											use subspace est for low-rank matrix completion of mrf data
% 											then recon matrix-completed mrf data and fit m0,t1,t2 per-voxel
% yang::lra 						fit m0,t1,t2,b0 from low-rank mrf im using coarse low-rank dict per-voxel
%												and then apply bilinear interpolation: x100 acceleration
Chapter~\ref{c,relax} applied
a common parameter estimation strategy to QMRI
that involves minimizing a cost function
related to a statistical likelihood function.
Because MR signal models are typically nonlinear functions
of the underlying latent parameters,
such likelihood-based estimation
usually requires non-convex optimization.
To seek good solutions,
many works
(\eg, 
\cite{%
	haldar:07:mle,%
	hernando:08:jeo,%
	barral:10:arm,%
	staroswiecki:12:seo,%
	ma:13:mrf,%
	trzasko:13:etf,%
	mcgivney:14:scf,%
	zhao:14:mbm,%
	beneliezer:15:raa,%
		zhao:15:amp,%
	cauley:15:fgm,%
	zhao:16:mlr,%
	nataraj:17:oms,%
	asslander::lra,%
	yang::lra%
})
approach estimation
with algorithms
that employ exhaustive grid search,
which requires either storing
or computing on-the-fly 
a ``dictionary'' of signal vectors.
These works estimate a small number (2-3)
of nonlinear latent parameters,
so grid search is practical.
However, 
for moderate or large sized problems,
the required number 
of dictionary elements
renders grid search undesirable or even intractable,
unless one imposes artificially restrictive latent parameter constraints.
Though several recent works
\cite{%
	mcgivney:14:scf,%
	cauley:15:fgm,%
	asslander::lra,%
	yang::lra%
}
focus on reducing dictionary storage requirements,
all of these methods ultimately rely 
on some form of dictionary-based grid search.

% clear need for a method that scales well with # parameters?
% multi-compartment: 6-11
% diffusion: at least 7
% phase-based methods: flow, b1, b0, asl? 
There are numerous QMRI applications
that could benefit from an alternative parameter estimation method
that scales well with the number of latent parameters.
For example,
vector (\eg, flow \cite{feinberg:85:mri})
and tensor 
(\eg, diffusivity \cite{bihan:01:dti} or conductivity \cite{tuch:01:ctm})
field mapping techniques
require estimation 
of at minimum 4 and 7 latent parameters per voxel,
respectively.
Phase-based longitudinal \cite{sekihara:85:nif} 
or transverse \cite{morrell:08:aps,sacolick:10:bmb} field mapping
could avoid noise-amplifying algebraic manipulations
on reconstructed image data
that are conventionally used
to reduce signal dependencies 
on nuisance latent parameters.
Compartmental fraction mapping \cite{mackay:94:ivv,nataraj:17:mwf}
from steady-state pulse sequences
requires estimation of at least 7 \cite{deoni:08:gmt}
and as many as 10 \cite{deoni:13:oct}
latent parameters per voxel.
In these and other applications,
greater estimation accuracy
requires more complete signal models
that involve more latent parameters,
increasing the need 
for scalable estimation methods.

% kernel methods
% fundamental challenge: nonlinear model
% classical idea: transform nonlinear problem into a linear one
% unclear how to balance complexity of transformation for accuracy with increase in dimensionality
% fortunately, simple transforms involving certain reproducing kernel functions yield solutions that need not scale in complexity with the dimensionality of the associated transformed data
% wahba introduced representation in approx theory
% scholkopf introduced in context of learning theory
% enjoyed success in many machine learning applications,
% originally for classification and later regression
The fundamental challenge 
of scalable MRI parameter estimation
stems from MR signal model nonlinearity:
standard linear estimators
would be scalable but inaccurate.
One natural solution strategy
involves nonlinearly preprocessing reconstructed images
such that the transformed images 
are at least approximately linear
in the latent parameters.
As an example,
for simple $\Tt$ estimation
from measurements at multiple echo times,
one could apply linear regression
to the logarithm of the measurements
(Section~\ref{s,demo} builds further intuition
using this simple application).
However,
such simple transformations
are generally not evident 
for more complicated signal models.
Without such problem-specific insight,
sufficiently rich nonlinear transformations
could dramatically increase problem dimensionality,
hindering scalability.
Fortunately, 
a celebrated result
in approximation theory \cite{kimeldorf:70:acb} showed
that simple transformations involving
\emph{reproducing kernel} functions \cite{aronszajn:50:tor}
can represent nonlinear estimators
whose evaluation need not directly scale in computation
with the (possibly very high) dimension
of the associated transformed data.
These kernel methods later found popularity
in machine learning
(initially for classification \cite{cortes:95:svn}
and quickly thereafter for other applications,
\eg, regression \cite{saunders:98:rrl})
because they provided simple, scalable nonlinear extensions
to fast linear algorithms.

% related work
The general idea
of using linearization
to simplify a nonlinear estimation problem
has been used before in QMRI.
For example,
orthogonal transforms
have been used
to linearly represent 
exponential \cite{huang:12:tmf}
and extended phase graph \cite{huang:13:trw} models
for $\Tt$ estimation.
An unscented Kalman filter 
has been used 
to linearly represent nonlinear models
for general multiple-parameter estimation
up to third-order accuracy \cite{zhao:16:daa}.
Whereas these prior works largely focus
on parameter estimation accuracy gains 
in under-sampled acquisitions,
this paper focuses on acceleration 
for general per-voxel MRI parameter estimation
from reconstructed images.

% idea here is to link parameter estimation 
% to a problem of regression
% ideas of machine learning can link estimation (seek parameter estimates from data assuming model) to regression (seek regression function relating inputs to outputs)
This chapter introduces  
a fast dictionary-free method
for MRI parameter estimation
via regression with kernels (PERK).
PERK first simulates many instances
of latent parameter inputs
and measurement outputs
using prior distributions
and a general nonlinear MR signal model.
PERK takes such input-output pairs
as simulated \emph{training points}
and then \emph{learns}
(using an appropriate nonlinear kernel function)
a nonlinear \emph{regression function}
from the training points.
PERK may scale considerably better
with the number of latent parameters
than likelihood-based estimation 
via grid search.

The remainder of this chapter
is organized as follows.
Section~\ref{s,perk,rev} reviews 
pertinent background information about kernels. 
Section~\ref{s,perk,meth} formulates 
a function optimization problem
for MRI parameter estimation
and efficiently solves this problem 
using kernels.
Section~\ref{s,perk,perf} studies bias and covariance
of the resulting PERK estimator.
Section~\ref{s,perk,pract} addresses practical implementation issues
such as computational complexity and model selection.
Section~\ref{s,perk,demo} provides intuition into PERK
through a simple toy problem.
Section~\ref{s,perk,exp} demonstrates PERK
in numerical simulations
as well as phantom and \invivo experiments.
Section~\ref{s,perk,disc} discusses advantages,
challenges, and extensions.
Section~\ref{s,perk,conc} summarizes key contributions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
\label{s,perk,rev}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This brief section reviews
relevant definitions and facts about kernels.
A (real-valued) \emph{kernel} 
$k : \setQ^2 \mapsto \real$
is a function 
that describes a measure of similarity
between two pattern vectors 
$\bmq,\bmq' \in \setQ$.
The matrix $\bmK \in \reals{N \times N}$
associated with kernel $k$
and $N \in \setN$ patterns $\bmq_1,\dots,\bmq_N \in \setQ$
consists of entries
$k\paren{\bmq_n,\bmq_{n'}}$
for $n,n' \in \set{1,\dots,N}$.
A \emph{positive definite kernel} is a kernel
for which $\bmK$ is positive semidefinite (PSD)
for any finite set of pattern vectors,
in which case $\bmK$
is a \emph{Gram matrix}.
A \emph{symmetric kernel} satisfies 
$k\paren{\bmq,\bmq'} = k\paren{\bmq',\bmq}$
$\forall \bmq,\bmq' \in \setQ$.
We hereafter restrict attention
to symmetric, positive definite (SPD) kernels.

An SPD kernel $k : \setQ^2 \mapsto \real$
defines an inner product 
in a particular Hilbert function space $\rkhs$
that we briefly describe here
because it characterizes
the class of candidate regression functions
over which PERK operates.
To envision $\rkhs$,
first define a kernel's associated \emph{(canonical) feature map} 
$\bmz : \setQ \mapsto \reals{\setQ}$
that assigns each $\bmq \in \setQ$ 
to a \emph{(canonical) feature} $k\paren{\cdot,\bmq} \in \reals{\setQ}$.
Then $\rkhs$ is a completion 
of the space $\setH := \set{\sum_{n=1}^N a_n k\paren{\cdot,\bmq_n}}$
spanned by point evaluations
of the feature map,
where
$N \in \setN$,
$a_1,\dots,a_N \in \real$,
and
$\bmq_1,\dots,\bmq_N \in \setQ$ are arbitrary.
Let $\innprod{\cdot}{\cdot} : \rkhs^2 \mapsto \real$ 
denote the inner product on $\rkhs$.
Then for any $h,h' \in \setH$
that have finite-dimensional canonical representations
$h := \sum_{n=1}^N a_n k\paren{\cdot,\bmq_n}$ 
and
$h' := \sum_{n'=1}^N b_{n'} k\paren{\cdot,\bmq_{n'}}$,
the assignment
\begin{align}
	\innprod{h}{h'}_\rkhs =
		\sum_{n=1}^N \sum_{n'=1}^N a_n b_{n'} k\paren{\bmq_{n'},\bmq_n}
	\label{eq,inn-prod}
\end{align}
is consistent
with the inner product on $\rkhs$.
This inner product exhibits $\forall h\in\rkhs, \bmq\in\setQ$
an interesting \emph{reproducing property}
\begin{align}
	\innprod{h}{k\paren{\cdot,\bmq}}_\rkhs = h\paren{\bmq}
	\label{eq,rep-prop}
\end{align}
that can be seen to directly follow 
from \eqref{eq,inn-prod}
for $h \in \setH$.

A \emph{reproducing kernel} (RK) is a kernel 
that satisfies \eqref{eq,rep-prop}
for some real-valued Hilbert space $\rkhs$.
A kernel is reproducing if and only if it is SPD.
There is a bijection between RK $k$ and $\rkhs$,
and so $\rkhs$ is often called
the \emph{reproducing kernel Hilbert space} (RKHS)
uniquely associated with RK $k$.
This bijection is critical
to practical function optimization over an RKHS
in that it translates inner products 
in a (usually high-dimensional) RKHS $\rkhs$
into equivalent kernel operations 
in the (lower-dimensional) pattern vector space $\setQ$.
The following sections exploit 
the bijection between an RKHS 
and its associated RK.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Function Optimization Problem \& Kernel Solution}
\label{s,perk,meth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After image reconstruction,
many QMRI acquisitions 
produce at each voxel position
a sequence of noisy measurements
$\bmy \in \complexes{D}$, 
modeled as
\begin{align}
	\bmy = \bmsa{\bmx, \bmnu} + \bmeps,
	\label{eq,model}
\end{align}
where $\bmx \in \reals{L}$ denotes $L$ \emph{latent} parameters;
$\bmnu \in \reals{K}$ denotes $K$ \emph{known} parameters;  
$\bms : \reals{L} \times \reals{K} \mapsto \complexes{D}$ 
models $D$ noiseless continuous signal functions;
and $\bmeps \sim \cgauss{\zeros{D}}{\bmSig}$ is complex Gaussian noise
with zero mean $\zeros{D} \in \reals{D}$
and known covariance $\bmSig \in \reals{D \times D}$.
(As a concrete example,
for $\Tt$ estimation
from single spin echo measurements,
$\bmx$ could collect spin density and $\Tt$;
$\bmnu$ could collect known longitudinal and transverse field inhomogeneities;
and 
$\bmy$ could collect measurements at $D$ echo times.)
We seek to estimate 
on a per-voxel basis
each latent parameter $\bmx$
from measurement $\bmy$ 
and known parameter $\bmnu$.

To develop an estimator $\est{\bmx}$,
we simulate many instances 
of forward model \eqref{eq,model}
and use kernels
to estimate a nonlinear inverse function.
We sample part of $\reals{L} \times \reals{K} \times \complexes{D}$
and evaluate \eqref{eq,model} $N$ times
to produce sets of object parameter and noise realizations
$\set{\paren{\bmx_1,\bmnu_1,\bmeps_1},\dots,\paren{\bmx_N,\bmnu_N,\bmeps_N}}$
and corresponding measurements
$\set{\bmy_1,\dots,\bmy_N}$. 
We seek a function
$\est{\bmh}~:~\reals{\dimQ} \mapsto \reals{L}$
and an offset $\est{\bmb} \in \reals{L}$
that together map each pure-real
\footnote{%
	We present our methodology 
	assuming pure-real patterns $\bmq$ 
	and estimators $\est{\bmx}$
	for simplicity and 
	to maintain consistency
	with experiments,
	in which we choose to use magnitude images
	for unrelated reasons 
	(see \S\ref{ss,exp,meth} for details). 
	It is straightforward 
	to generalize Theorem~\ref{thm,rep}
	for complex-valued kernels 
	and thereby address the cases 
	of complex patterns and/or estimators.
}
regressor $\bmq_n := [\abs{\bmy_n}\tpose, \bmnu_n\tpose]\tpose$
to an estimate 
$\bmxha{\bmq_n} := \bmhha{\bmq_n}+\est{\bmb}$ 
that is ``close'' 
to corresponding regressand $\bmx_n$,
where $\dimQ := D+K$,
$n \in \set{1,\dots,N}$,
and $\paren{\cdot}\tpose$ denotes vector transpose.
For any finite $N$,
there are infinitely many candidate estimators
that are consistent with training points
in this manner.
We use function regularization
to choose one estimator
that smoothly interpolates 
between training points:
\begin{align}
	\paren{\est{\bmh},\est{\bmb}} &\in 
		\argmin{\substack{\bmh \in \rkhs^L \\ \bmb \in \reals{L}}}
		\costa{\bmh, \bmb; \set{\paren{\bmx_n,\bmq_n}}_{1}^N}, 
		\where \label{eq,prob} \\
	\costa{\bmh, \bmb; \set{\paren{\bmx_n,\bmq_n}}_{1}^N} &= 
		\sum_{l=1}^L \cost_l\paren{h_l,b_l; \set{\paren{x_{l,n},\bmq_n}}_{1}^N}; 
		\label{eq,cost} \\
	\Psi_l(h_l,b_l; \set{\paren{x_{l,n},\bmq_n}}_{1}^N) &= 
		\rho_l \norm{h_l}_\rkhs^2 + 
		\frac{1}{N} \sum_{n=1}^N \paren{h_l(\bmq_n) + b_l - x_{l,n}}^2.
		\label{eq,cost-l}
\end{align}
Here, each $h_l~:~\reals{\dimQ} \mapsto \real$ is a scalar function
that maps to the $l$th component of the output of $\bmh$; 
each $b_l,x_{l,n} \in \real$ are scalar components of $\bmb,\bmx_n$;
$\rkhs$ is an RKHS 
whose norm $\norm{\cdot}_\rkhs$ 
is induced by inner product 
$\innprod{\cdot}{\cdot}_\rkhs : \rkhs^2 \mapsto \real$; 
and each $\rho_l$ controls for regularity in $h_l$.

Since \eqref{eq,cost} is separable 
in the components of $\bmh$ and $\bmb$, 
it suffices to consider optimizing each $\paren{h_l,b_l}$ 
by separately minimizing \eqref{eq,cost-l} 
for each $l \in \set{1,\dots,L}$.
Remarkably,
a generalization of the Representer Theorem \cite{scholkopf:01:agr},
restated as is relevant here for completeness,
reduces minimizing \eqref{eq,cost-l} 
to a finite-dimensional optimization problem.
\begin{thm}[Generalized Representer, \cite{scholkopf:01:agr}]
	Define $k : \reals{Q} \times \reals{Q} \mapsto \real$
	to be the SPD kernel 
	associated with RKHS $\rkhs$, 
	such that reproducing property $h_l(\bmq) = \innprod{h_l}{k(\cdot,\bmq)}_\rkhs$
	holds for all $h_l \in \rkhs$ and $\bmq \in \reals{Q}$. 
	Then any minimizer $(\est{h}_l,\est{b}_l)$ of \eqref{eq,cost-l}
	over $\rkhs \times \real$
	admits a representation for $\est{h}_l$ of the form
	\label{thm,rep}
	\begin{align}
		\est{h}_l(\cdot) \equiv \sum_{n=1}^N a_{l,n} k(\cdot,\bmq_{n}),
		\label{eq,rep}
	\end{align}
	where each $a_{l,n} \in \real$ for $n \in \set{1,\dots,N}$.
\end{thm}

Thm.~\ref{thm,rep} ensures 
that any solution
to the component-wise 
$\paren{N+1}$-dimensional problem
\begin{align}
	(\est{\bma}_l,\est{b}_l) \in 
	&\argmin{\substack{\bma_l \in \reals{N} \\ b_l \in \real}} 
	\rho_l \norm{\sum_{n'=1}^N a_{l,n'} k(\cdot,\bmq_{n'})}^2_\rkhs + \nonumber \\
	&\frac{1}{N} \sum_{n=1}^N \paren{\sum_{n'=1}^N a_{l,n'} k(\bmq_n,\bmq_{n'}) + b_l - x_{l,n}}^2
	\label{eq,cvx}
\end{align}
corresponds via \eqref{eq,rep} 
to a minimizer of \eqref{eq,cost-l}
over $\rkhs \times \real$,
where $\bma_l := [a_{l,1},\dots,a_{l,N}]\tpose$.
Fortunately, a solution of \eqref{eq,cvx} exists uniquely
for $\rho_l > 0$
and can be expressed as
\begin{align}
	\est{\bma}_l &= \inv{\paren{\bmM \bmK \bmM + N\rho_l\eye{N}}} \bmM \bmxtl{l};
	\label{eq,a-hat} \\
	\est{b}_l &= \frac{1}{N} \ones{N}\tpose \paren{\bmxtl{l} - \bmK \est{\bma}_l},
	\label{eq,b-hat}
\end{align}
where 
$\bmK \in \reals{N \times N}$ is the Gram matrix 
consisting of entries $k(\bmq_n,\bmq_{n'})$ for $n,n' \in \set{1,\dots,N}$;
$\bmM := \eye{N}-\frac{1}{N}\ones{N}\ones{N}\tpose \in \reals{N \times N}$
is a de-meaning operator;
$\bmxtl{l} := [x_{l,1},\dots,x_{l,N}]\tpose$;
$\eye{N} \in \reals{N \times N}$ is the identity matrix;
and $\ones{N} \in \reals{N}$ is a vector of ones.
Substituting \eqref{eq,a-hat} into \eqref{eq,rep} 
yields an expression 
for the $l$th entry $\est{x}_l$ 
of MRI parameter estimator $\est{\bmx}$:
\begin{align}
	\est{x}_l\paren{\cdot} &\gets \bmxtl{l}\tpose 
		\paren{\frac{1}{N}\ones{N} + 
		\bmM\inv{\paren{\bmM\bmK\bmM + N\rho_l\eye{N}}} \bmka{\cdot}},
		\label{eq,xl-hat}
\end{align}
where
$\bmka{\cdot} := 
	\brac{k(\cdot,\bmq_1),\dots,k(\cdot,\bmq_N)}\tpose - \frac{1}{N}\bmK\ones{N}
	: \reals{Q} \mapsto \reals{N}$
is an embedding operator.

% utility of estimate depends on kernel function
% valid kernel is q'*q, which corresponds to linear ridge regression
% more useful kernel is nonlinear fun of q; we use gaussian
When $\rho_l>0\,\, \forall l \in \set{1,\dots,L}$, 
estimator $\bmxha{\cdot}$
with entries \eqref{eq,xl-hat} minimizes \eqref{eq,cost}
over $\rkhs^L \times \real^L$.
However, the utility of $\bmxha{\cdot}$
depends on the choice of kernel $k$,
which induces a choice on the RKHS $\rkhs$
and thus the function space $\rkhs^L \times \real^L$
over which \eqref{eq,prob} optimizes.
For example, if $k$ was selected as the canonical dot product 
$k(\bmq,\bmq') \gets \innprod{\bmq}{\bmq'}_{\reals{Q}} := \bmq\tpose \bmq'$
(for which RKHS $\rkhs \gets \reals{Q}$),
then \eqref{eq,xl-hat} would reduce 
to affine ridge regression \cite{hoerl:70:rrb}
which is optimal over $\reals{Q} \times \real$
but is unlikely to be useful 
when signal model $\bms$ is nonlinear in $\bmx$.
Since we expect a useful estimate $\est{\bmx}\paren{\bmq}$ 
to depend nonlinearly (but smoothly) 
on $\bmq$ in general, 
we instead use 
an SPD kernel $k$ 
that is likewise nonlinear in its arguments
and thus corresponds to an RKHS much richer than $\reals{Q}$. 
Specifically, we use a Gaussian kernel
\begin{align}
	k(\bmq,\bmq') \gets \expa{-\frac{1}{2}{\norm{\bmq-\bmq'}^2_{\bmL^{-2}}}},
	\label{eq,kern}
\end{align}
where positive definite matrix bandwidth $\bmL \in \reals{Q \times Q}$ 
controls the length scales in $\bmq$ over which 
the estimator $\est{\bmx}$ smooths
and $\norm{\cdot}_{\bmG} \equiv \norm{\bmG^{1/2}\paren{\cdot}}_2$
is a weighted $\ell^2$-norm
with PSD matrix weights $\bmG$.
We use a Gaussian kernel
over other candidates
because it is a \emph{universal kernel},
meaning weighted sums of the form 
$\sum_{n=1}^N a_n k\paren{\cdot,\bmq_n}$
can approximate $\Ltwo$ functions
to arbitrary accuracy
for $N$ sufficiently large
\cite{steinwart:08:svm}.

Interestingly, 
the RKHS associated 
with Gaussian kernel \eqref{eq,kern}
is infinite-dimensional.
Thus, 
Gaussian kernel regression
can be interpreted as 
first ``lifting'' 
via a nonlinear \emph{feature map} 
$\bmz : \reals{Q} \mapsto \rkhs$ 
each $\bmq$ 
into an infinite-dimensional \emph{feature} 
$\bmza{\bmq} = k\paren{\cdot,\bmq} \in \rkhs$,
and then performing regularized affine regression
on the features
via dot products of the form
$\innprod{k\paren{\cdot,\bmq}}{k\paren{\cdot,\bmq'}}_{\rkhs}
	= k\paren{\bmq',\bmq}$.
From this perspective,
the challenges of nonlinear estimation 
via likelihood models
are avoided 
because we \emph{select} 
(through the choice of kernel) 
characteristics of the nonlinear dependence
that we wish to model
and need only \emph{estimate} via \eqref{eq,cvx} 
the linear dependence
of each entry in $\est{\bmx}$ 
on the corresponding features.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias and Covariance Analysis}
\label{s,perk,perf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section presents expressions
for the bias and covariance
of Gaussian PERK estimator $\esta{\bmx}{\cdot}$,
conditioned on object parameters $\bmx,\bmnu$.
We focus on these conditional statistics
to enable study
of estimator performance 
as $\bmx,\bmnu$ are varied.
Though not mentioned explicitly hereafter,
both expressions treat the training sample
$\set{\paren{\bmx_1,\bmq_1},\dots,\paren{\bmx_N,\bmq_N}}$
and regularization parameters $\rho_1,\dots,\rho_L$
as fixed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conditional Bias}
\label{ss,perk,perf,bias}

The conditional bias 
of $\est{\bmx} \equiv \esta{\bmx}{\bmymag,\bmnu}$
is written as
\begin{align}
	\biasa{\est{\bmx}|\bmx,\bmnu} 
		&:= 
			\expect{\bmymag|\bmx,\bmnu}{\esta{\bmx}{\bmymag,\bmnu}} - \bmx
			\nonumber \\
		&= 
			\bmR \expect{\bmymag|\bmx,\bmnu}{\bmka{\bmymag,\bmnu}} 
			+ \paren{\bmmx-\bmx},
			\label{eq,bias}
\end{align}
where $\expect{\bmymag|\bmx,\bmnu}{\cdot}$ 
denotes expectation
with respect to $\bmymag := \abs{\bmy}$
and conditioned on $\bmx,\bmnu$.
Here,
the $l$th row of $\bmR \in \reals{L \times N}$ 
and $l$th entry of regressand sample mean 
$\bmmx \in \reals{L}$
respectively are $\bmxt{l}\tpose \bmM\inv{\paren{\bmM\bmK\bmM + N \rho_l \eye{N}}}$
and $\frac{1}{N}\bmxt{l}\tpose\ones{N}$
for $l \in \set{1,\dots,L}$.
To proceed analytically, 
we make two mild assumptions.
First, 
we assume 
that $\bmy \sim \cgauss{\zeros{D}}{\bmSig}$
has sufficiently high signal-to-noise ratio (SNR)
such that its complex modulus $\bmymag$ 
is approximately Gaussian-distributed.
We specifically consider the typical case
where covariance matrix $\bmSig$ is diagonal
with diagonal entries $\sigma_1^2,\dots,\sigma_D^2$,
in which case measurement amplitude conditional distribution 
$\dist{\bmymag|\bmx,\bmnu}$
is simply approximated as
$\dist{\bmymag|\bmx,\bmnu} \gets \gauss{\bmmu}{\bmSig}$,
where $\bmmu \in \reals{D}$ 
has $d$th coordinate $\sqrt{\abs{s_d\paren{\bmx,\bmnu}}^2 + \sigma_d^2}$ 
for $d \in \set{1,\dots,D}$ \cite{gudbjartsson:95:trd}.
Second,
we assume
that the Gaussian kernel bandwidth matrix $\bmL$ 
has the block diagonal structure
\begin{align}
	\bmL \gets 
		\begin{bmatrix}
			\bmLy & \zeros{D \times K} \\
			\zeros{K \times D} & \bmLnu.
		\end{bmatrix}
	\label{eq,sep-bw}
\end{align}
where $\bmLy \in \reals{D \times D}$
and $\bmLnu \in \reals{K \times K}$ 
are positive definite.
With these simplifying assumptions,
the $n$th entry 
of the expectation in \eqref{eq,bias}
is well approximated as
\begin{align}
	\brac{\expect{\bmymag|\bmx,\bmnu}{\bmka{\bmymag,\bmnu}}}_n
		&=
			\int_{\reals{D}} e^{%
				-\frac{1}{2} \norm{\bmq-\bmq_n}^2_{\bmL^{-2}}
			}% 
			\dista{\bmymag|\bmx,\bmnu} \der{\bmymag}
			\nonumber \\
		&\approx	
			\frac{e^{-\frac{1}{2}\norm{\bmnu-\bmnu_n}^2_{\bmLnu^{-2}}}}
			{\sqrt{\paren{2\pi}^D \det\paren{\bmSig}}}
			\int_{\reals{D}} e^{%
				-\frac{1}{2} \paren{%
					\norm{\bmymag-\bmymag_n}^2_{\bmLy^{-2}} + 
					\norm{\bmymag-\bmmu}^2_{\bmSig^{-1}}
				}%
			}%
			\der{\bmymag}
			\nonumber \\
		&=
			\frac{%
				e^{%
					-\frac{1}{2}\paren{%
						\norm{\bmnu-\bmnu_n}^2_{\bmLnu^{-2}} +
						\norm{\bmmu-\bmymag_n}^2_{\paren{\bmLy^{-2}\bmSig + \eye{D}}^{-1}\bmLy^{-2}}
					}%
				}
			}{%
				\sqrt{\det\paren{\bmLy^{-2}\bmSig + \eye{D}}}
			},
			\label{eq,exp-bmk}
\end{align}
where $\det\paren{\cdot}$ denotes determinant
and the Gaussian integral follows 
after completing the square
of the integrand's exponent.
It is clear 
from \eqref{eq,exp-bmk}
that as $\bmSig \to \zeros{D\times D}$
for fixed $\bmLy$,
$\expect{\bmymag|\bmx,\bmnu}{\bmka{\bmymag,\bmnu}}
	\to \bmka{\bmmu,\bmnu}$
and therefore
\begin{align}
	\expect{\bmymag|\bmx,\bmnu}{\esta{\bmx}{\bmymag,\bmnu}}
		\to \esta{\bmx}{\expect{\bmymag|\bmx,\bmnu}{\bmymag},\bmnu}
		\equiv \esta{\bmx}{\bmmu,\bmnu}
\end{align}
which perhaps surprisingly means that 
the conditional bias asymptotically approaches 
the noiseless conditional estimation error $\esta{\bmx}{\bmmu,\bmnu}-\bmx$
despite $\est{\bmx}$ being nonlinear in $\bmymag$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conditional Covariance}
\label{ss,perk,perf,cov}

The conditional covariance
of $\est{\bmx} \equiv \esta{\bmx}{\bmymag,\bmnu}$ 
is written as
\begin{align}
	\cova{\est{\bmx}|\bmx,\bmnu} 
		&:= 
			\expect{\bmymag|\bmx,\bmnu}{%	
				\paren{\est{\bmx}-\expect{\bmymag|\bmx,\bmnu}{\est{\bmx}}}
				\paren{\est{\bmx}-\expect{\bmymag|\bmx,\bmnu}{\est{\bmx}}}\tpose
			}%
			\nonumber \\
		&= 
			\bmR \expect{\bmymag|\bmx,\bmnu}{%
				\bmkta{\bmymag,\bmnu}\bmkta{\bmymag,\bmnu}\tpose
			}
			\bmR\tpose,	
			\label{eq,cov}
\end{align}
where $\bmkta{\bmymag,\bmnu} 
	:= \bmka{\bmymag,\bmnu} 
	- \expect{\bmymag|\bmx,\bmnu}{\bmka{\bmymag,\bmnu}}$.
To proceed analytically,
we take the same high-SNR 
and block-diagonal bandwidth assumptions
as in Subsection~\ref{ss,perk,perf,bias}.
Then after straightforward manipulations
similar to those yielding \eqref{eq,exp-bmk},
the $\paren{n,n'}$th entry 
of the expectation in \eqref{eq,cov}
is well approximated as
\begin{IEEEeqnarray}{rCl}
	\IEEEeqnarraymulticol{3}{l}{%
		\brac{%
			\expect{\bmymag|\bmx,\bmnu}{%
				\bmkta{\bmymag,\bmnu}\bmkta{\bmymag,\bmnu}\tpose
			}%
		}_{n,n'}%
		=
			e^{%
				-\frac{1}{2}\paren{%
					\norm{\bmnu-\bmnu_n}^2_{\bmLnu^{-2}} + 
					\norm{\bmnu-\bmnu_{n'}}^2_{\bmLnu^{-2}}
				} 
			}%
	}%
	\nonumber \\*
		&\times& 
			\paren{%
				\frac{%
					e^{%
						-\frac{1}{2}\paren{%
							\norm{\bmymagt_n-\bmymagt_{n'}}^2_{\bmDeltaa{0}} +
							\norm{\bmymagt_n+\bmymagt_{n'}}^2_{\bmDeltaa{2}}
						}%
					}%
				}{%
					\sqrt{\det\paren{2\bmLy^{-2}\bmSig + \eye{D}}}
				}%
				-
				\frac{%
					e^{%
						-\frac{1}{2}\paren{%
							\norm{\bmymagt_n-\bmymagt_{n'}}^2_{\bmDeltaa{1}} +
							\norm{\bmymagt_n+\bmymagt_{n'}}^2_{\bmDeltaa{1}}
						}%
					}%
				}{%
					\det\paren{\bmLy^{-2}\bmSig + \eye{D}}
				}%
			},%
	\label{eq,exp-bmkkt}
\end{IEEEeqnarray}
where $\bmymagt_n := \bmmu-\bmymag_n$
and $\bmDeltaa{t} := 
	\frac{1}{2}\inv{\paren{t\bmLy^{-2}\bmSig + \eye{D}}} \bmLy^{-2}$
for $t \in \setN$.
The emergence 
of $\bmymagt_n \pm \bmymagt_{n'}$ terms
in \eqref{eq,exp-bmkkt} 
show that the conditional covariance 
(unlike the conditional bias) 
is directly influenced
not only by the individual expected test point distances
to each of the training points
$\bmymagt_1,\dots,\bmymagt_N$
but also by the local training point sampling density.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Considerations}
\label{s,perk,pract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section focuses 
on important practical implementation issues.
Subsection~\ref{ss,perk,pract,apprx} discusses
a conceptually intuitive approximation
of PERK estimator \eqref{eq,xl-hat}
that in many problems
can significantly improve computational performance.
Subsection~\ref{ss,perk,pract,mod} describes strategies 
for data-driven model selection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Kernel Approximation}
\label{ss,perk,pract,apprx}

In practical problems
with even moderately large ambient dimension $\dimQ$,
the necessarily large number of training samples $N$ 
complicates storage of (dense) $N\times N$ Gram matrix $\bmK$.
Using a kernel approximation 
can mitigate storage and processing issues.
Here we employ \emph{random Fourier features} \cite{rahimi:07:rff},
a recent method 
for approximating translation-invariant kernels
having form $k\paren{\bmq,\bmq'} \equiv k\paren{\bmq-\bmq'}$.
This subsection reviews the main result of \cite{rahimi:07:rff}
for the purpose of constructing 
an intuitive and computationally efficient approximation 
of \eqref{eq,xl-hat}.

The strategy of \cite{rahimi:07:rff}
is to construct independent probability distributions 
$\dist{\bmv}$ and $\dist{s}$
associated with
random $\bmv \in \reals{\dimQ}$ 
and random $s \in \real$ 
as well as a function 
(that is parameterized by $\bmq$) 
$\zt\paren{\cdot,\cdot;\bmq} : 
	\reals{\dimQ} \times \real \times \reals{\dimQ} \mapsto \real$,
such that
\begin{align}
	\expect{\bmv,s}{
		\zta{\bmv,s;\bmq}
		\zta{\bmv,s;\bmq'} 
	}
	= k(\bmq-\bmq'),
	\label{eq,exp}
\end{align}
where 
$\expect{\bmv,s}{\cdot}$
denotes expectation with respect to $\dist{\bmv}\dist{s}$.
When such a construction exists,
one can build
approximate feature maps $\bmztZ$
by concatenating and normalizing evaluations 
of $\zt$ 
on $Z$ samples 
$\set{\paren{\bmv_1,s_1},\dots,\paren{\bmv_Z,s_Z}}$
of $\paren{\bmv,s}$
(drawn jointly albeit independently),
to produce approximate features
\begin{align}
	\bmztZa{\bmq} := \sqrt{\frac{2}{Z}}
		\brac{\zta{\bmv_1,s_1;\bmq},\dots,\zta{\bmv_Z,s_Z;\bmq}}\tpose
	\label{eq,feat}
\end{align}
for any $\bmq$. 
Then by the strong law of large numbers,
\begin{align}
	\lim_{Z \to \infty} \innprod{
		\bmztZa{\bmq}}{
		\bmztZa{\bmq'}
	}_{\reals{Z}} \overset{a.s.}{\to} k(\bmq,\bmq') \qquad \forall \bmq,\bmq'
	\label{eq,lln}
\end{align}
which, 
in conjunction 
with strong performance guarantees
for finite $Z$ \cite{rahimi:07:rff,sutherland:15:ote}, 
justifies interpreting $\bmztZ$ 
as an approximate 
(and now finite-dimensional) feature map.

We use the Fourier construction 
of \cite{rahimi:07:rff}
that assigns
$\zt\paren{\bmv,s;\bmq} 
\gets 
\cosa{2\pi\paren{\bmv\tpose \bmq + s}}$.
If also $\dist{s}\gets\unif{0,1}$, 
then 
$\expect{\bmv,s}{
	\zt\paren{\bmv,s;\bmq}
	\zt\paren{\bmv,s;\bmq'} 
}$
simplifies to
\begin{align}
	\int_{\reals{\dimQ}} \cosa{2\pi\bmv\tpose\paren{\bmq-\bmq'}} \dist{\bmv}\paren{\bmv} \der{\bmv}.
	\label{eq,ft}
\end{align}
For symmetric $\dist{\bmv}$,
\eqref{eq,ft} exists \cite{wu:97:gbt}
and is a Fourier transform.
Thus choosing 
$\dist{\bmv}\gets\gauss{\zeros{\dimQ}}{\paren{2\pi\bmL}^{-2}}$
satisfies \eqref{eq,exp}
for Gaussian kernel \eqref{eq,kern},
where $\zeros{\dimQ} \in \reals{\dimQ}$ is a vector of zeros.

Sampling $\dist{\bmv},\dist{s}$ $Z$ times
and subsequently constructing 
$\bmZt := 
	\brac{\bmztZa{\bmq_1},\dots,\bmztZa{\bmq_N}} \in \reals{Z \times N}$
via repeated evaluations of \eqref{eq,feat}
gives for $Z \ll N$
a low-rank approximation $\bmZt\tpose\bmZt$ 
of Gram matrix $\bmK$.
Substituting this approximation into \eqref{eq,xl-hat}
and applying the matrix inversion lemma \cite{woodbury:50:imm} 
yields
\begin{align}
	\est{x}_l\paren{\cdot} \gets \mxl + 
		\cztxl\tpose\inv{\paren{\Cztzt + \rho_l\eye{Z}}} \paren{\bmztZa{\cdot}-\bmmzt},
	\label{eq,xl-apx}
\end{align}
where 
$\mxl := \frac{1}{N}\bmxt{l}\tpose\ones{N}$ 
and 
$\bmmzt := \frac{1}{N}\bmZt\ones{N}$ 
are sample means; 
and
$\cztxl := \frac{1}{N}\bmZt\bmM\bmxt{l}$
and 
$\Cztzt := \frac{1}{N}\bmZt\bmM\bmZt\tpose$ 
are sample covariances.
Estimator \eqref{eq,xl-apx} 
is an affine minimum mean-squared error estimator
on the approximate features,
and illustrates
that Gaussian PERK 
via estimator \eqref{eq,xl-hat}
is asymptotically (in $Z$) equivalent
to regularized affine regression
after nonlinear, high-dimensional feature mapping.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tuning Parameter Selection}
\label{ss,perk,pract,mod}

This subsection proposes guidelines
for data-driven selection
of user-selectable parameters.
Our goal here is
to use problem intuition
to automatically choose 
as many tuning parameters as possible,
thereby leaving as few parameters as possible
to manual selection.
In this spirit,
we focus on ``online'' model selection,
where one chooses tuning parameters
for training the estimator $\esta{\bmx}{\cdot}$
\emph{after} acquiring (unlabeled) real test data.
This online approach 
can be considered a form 
of \emph{transductive learning} \cite[Ch.~8]{vapnik:98:slt}
since we train our estimator
with knowledge of unlabeled test data
in addition to labeled training data.
Observe that since
many voxel-wise separable MRI parameter estimation problems
are comparatively low-dimensional,
PERK estimators can often be quickly trained
using only a moderate number
of simulated training examples;
in fact,
training can in some problems take
comparable or even less time
than evaluating the PERK estimator
on full-volume high-resolution measurement images.
For these reasons,
online PERK model selection is often practical.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Choosing Sampling Distribution}
\label{sss,perk,pract,mod,dist}

% for good krr performance, want parameters well-identified over samp dist
% we quantify identifiability using fisher matrix
% fisher inverse lower-bounds cov for any unbiased est
% ml estimator is one estimator that asymptotically achieves crb 
% with above assumptions, ml is continuous in y,nu
% thus when crb well behaved there exists one cts estimator identifies x from y,nu
% universal kernels can approx any cts function to arbitrary accuracy 
% ...

For reasonable PERK performance,
it is important 
to choose the joint distribution
of latent and known parameters $\dist{\bmx,\bmnu}$
such that latent parameters
can be estimated precisely
over the joint distribution's support $\supp{\dist{\bmx,\bmnu}}$. 
For continuously differentiable 
magnitude signal model $\bmmu$,
we quantify precision
at a single point $\paren{\bmx,\bmnu}$
using the Fisher information matrix
\begin{IEEEeqnarray}{rCl}
	\bmFa{\bmx,\bmnu} 
		&:=& 
			\expect{\bmymag|\bmx,\bmnu}{%
				\paren{\grada{\bmx}\log{\dist{\bmymag|\bmx,\bmnu}}}\tpose 
				\grada{\bmx}\log{\dist{\bmymag|\bmx,\bmnu}}
			}% 
			\nonumber \\	
		&\approx& 	
			\paren{\grada{\bmx}{\bmmu\paren{\bmx,\bmnu}}}\tpose
				\inv{\bmSig} \grada{\bmx}{\bmmu\paren{\bmx,\bmnu}}
	\label{eq,fisher}
\end{IEEEeqnarray}
where $\grada{\bmx}\paren{\cdot}$ denotes row gradient
with respect to $\bmx$
and the approximation holds well
for moderately high-SNR measurements \cite{gudbjartsson:95:trd}.
When it exists,
the inverse of $\bmFa{\bmx,\bmnu}$ 
provides a lower-bound
on the conditional covariance
of any unbiased estimator 
of $\bmx$ \cite{cramer:46}.
For good performance,
it is thus reasonable
to ensure $\bmFa{\bmx,\bmnu}$ is well-conditioned
over $\supp{\dist{\bmx,\bmnu}}$.

There are many strategies one could employ
to control the condition number 
of $\bmFa{\bmx,\bmnu}$ 
over $\supp{\dist{\bmx,\bmnu}}$.
In our experiments,
we used data \cite{nataraj:17:oms}
from acquisitions designed 
to \emph{minimize} a cost function
related to the \emph{maximum}
of $\bmF^{-1}\paren{\bmx,\bmnu}$
over bounded latent and known parameter ranges of interest
(Subsection~\ref{ss,perk,exp,meth} provides application-specific details).
We then assigned $\supp{\dist{\bmx,\bmnu}}$
to coincide with the support 
of these acquisition design parameter ranges of interest.
Assessing worst-case imprecision
via the conservative minimax criterion is appropriate here
because point-wise poor conditioning
at any $\paren{\bmx,\bmnu} \in \supp{\dist{\bmx,\bmnu}}$
can induce PERK estimation error 
over larger subsets 
of $\supp{\dist{\bmx,\bmnu}}$.
% tradeoff between volume of support over which we expect accuracy vs precision within support
% actually only true if max achieved at boundary

If many separate prior parameter estimates are available,
one can estimate the particular shape 
of $\dist{\bmx,\bmnu}$ empirically 
and then clip and renormalize $\dist{\bmx,\bmnu}$
so as to assign nonzero probability
only within an appropriate support.
When prior estimates are unavailable,
it may in certain problems be reasonable 
to instead assume
a separable distributional structure
$\dist{\bmx,\bmnu} \equiv \dist{\bmx}\dist{\bmnu}$
in which case
one can still estimate $\dist{\bmnu}$ empirically
but must set $\dist{\bmx}$ manually
based on typical ranges
of latent parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Choosing Regularization Parameters}
\label{sss,perk,pract,mod,reg}

% as presented krr est 11 and approx 17 provide freedom to choose rho diff for rho1 to rhoL
% however, the unitless matrices K and Czz whose conditionalities they improve before inversion do not change across parameters
% thus it is reasonable to set rho1 to rhoL to a fixed rho>0 across latent parameters
% added benefit of requiring only one NxN or ZxZ matrix inversion for training respectively, an acceleration especially helpful for online training
As presented,
PERK estimator \eqref{eq,xl-hat}
and its approximation \eqref{eq,xl-apx}
leave freedom to select different regularization parameters
$\rho_1,\dots,\rho_L$
for estimating each of the $L$ latent parameters.
However,
the respective unitless matrices 
$\bmM\bmK\bmM$ and $\Cztzt$ 
whose condition numbers are influenced 
by $\rho_1,\dots,\rho_L$
do not vary with $l$.
Thus it is reasonable 
to assign each $\rho_l \gets \rho\,\, \forall l \in \set{1,\dots,L}$ 
some fixed $\rho > 0$.
This simplification significantly reduces training computation
to just one
rather than $L$
large matrix inversions.
We select the scalar regularization parameter $\rho$
using the holdout process 
described in Subsection~\ref{s,holdout}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Choosing Kernel Bandwidth}
\label{sss,perk,pract,mod,length}

It is desirable 
to choose the Gaussian kernel's bandwidth matrix $\bmL$ 
such that PERK estimates are invariant 
to the overall scale of test data. 
We use
(after observing test data,
and for both training and testing)
\begin{align}
	\bmL \gets \lambda \diag{\brac{\bmmybar\tpose, \bmmnu\tpose}\tpose},
	\label{eq,bw}
\end{align}
where 
$\bmmybar \in \reals{D}$ 
and 
$\bmmnu \in \reals{K}$
are sample means across voxels
of magnitude test image data
and known parameters,
respectively;
and $\diag{\cdot}$ assigns its argument 
to the diagonal entries
of an otherwise zero matrix.
We select the
only scalar bandwidth parameter $\lambda>0$
using holdout as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PERK Demonstration in a 1-D Toy Problem}
\label{s,perk,demo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!ht]
	\centering
	\subfigure[$N\gets10$]{%
		\includegraphics [width=0.47\textwidth]{%
			toy-1d,t2-se,n-10%
		}
		\label{fig,toy,n-10}
	}
	\hspace{0.3cm}
	\subfigure[$N\gets20$]{%
		\includegraphics [width=0.47\textwidth] {%
			toy-1d,t2-se,n-20%
		}
		\label{fig,toy,n-20}
	}
	
	\subfigure[$N\gets50$]{%
		\includegraphics [width=0.47\textwidth]{%
			toy-1d,t2-se,n-50%
		}
		\label{fig,toy,n-50}
	}
	\hspace{0.3cm}
	\subfigure[$N\gets200$]{%
		\includegraphics [width=0.47\textwidth] {%
			toy-1d,t2-se,n-200%
		}
		\label{fig,toy,n-200}
	}
	\caption{%
		Illustrations of PERK 
		for $\Tt$ estimation
		from a single spin echo measurement.
		Subfigures vary the number $N$
		of PERK training points,
		marked with black circles.
		The orange and yellow curves plot
		PERK $\TtPERK$ and MOM $\TtMOM$ estimators
		evaluated at test points,
		marked with blue dots.
		Dashed black lines denote 
		the sampling distribution support $\supp{\dist{\Tt}}$
		over which each PERK estimator was trained.
		As $N$ increases,
		$\TtPERK$ appears more similar to $\TtMOM$
		within well-sampled regions
		of $\supp{\dist{\Tt}}$.
	}%
	\label{fig,toy}
\end{figure}

To build intuition
and for ease of visualization,
we first apply PERK
in a one-dimensional toy problem,
namely $\Tt$ estimation
from a single spin-echo measurement.
We generated training data
using a mono-exponential (unity-$\mzero$) signal model
$y = e^{-\TE/\Tt} + \epsilon$,
where $y$ is a complex spin-echo measurement,
$\TE \gets 30$ms is the echo time
and $\epsilon \sim \cgauss{0}{0.01^2}$ is complex Gaussian noise.
We sampled $N \gets 10$, $20$, $50$, $200$ regressands
from $\Tt$ sampling distribution $\dist{\Tt} \gets \logunif{10,500}$
and took the magnitude 
of noisy complex signal model evaluations
to generate corresponding magnitude regressors.
We trained PERK separately
using each of the four labeled training datasets,
holding fixed hyperparameters $(\lambda,\rho) \gets (2^{-1.5},2^{-20})$
that were manually chosen
to aid in illustrating PERK's typical behavior.
	
Fig.~\ref{fig,toy} illustrates the 1-D PERK estimator $\TtPERK$
and shows how its performance improves
as $N$ is increased. 
To produce each subfigure,
we uniformly sampled 100,000 true (latent) $\Tt$ values,
evaluated the noisy signal model as in training
to generate magnitude test points (blue dots),
and evaluated each PERK estimator 
at the unlabeled test points (orange curves).
For comparison,
subfigures within Fig.~\ref{fig,toy} also plot
the intuitive method-of-moments (MOM) estimator
$\TtMOM\paren{\cdot} := -\TE/\log{\abs{\cdot}}$ (yellow curves).
As $N$ increases,
$\TtPERK$ appears more similar to $\TtMOM$
within well-sampled regions of $\supp{\dist{\Tt}}$ 
(marked by dashed black lines).
PERK will be more useful
in nonlinear estimation problems
where such a minimally biased 
and low-dimensional MOM estimator is unavailable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimentation}
\label{s,perk,exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robustness Studies}
\label{s,perk,robust}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% a. mismatch noise var
% b. bad latent parameter distributions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{s,perk,disc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{s,perk,conc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
