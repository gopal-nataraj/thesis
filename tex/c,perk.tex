% mri parameter estimation via regression with kernels (PERK)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{s,perk,intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In quantitative magnetic resonance imaging (QMRI),
one seeks to estimate latent parameter images 
from suitably informative data.
Since MR acquisitions are tunably sensitive 
to many physical processes
(\eg, relaxation \cite{bloch:1946:ni-paper}, 
diffusion \cite{torrey:56:bew},
and chemical exchange \cite{mcconnell:58:rrb}),
MRI parameter estimation is important
for many QMRI applications
(\eg, relaxometry \cite{bloembergen:1948:rei}, 
diffusion tensor imaging \cite{bihan:01:dti}, 
and multi-compartmental imaging \cite{mackay:94:ivv}). 
Motivated by widespread applications,
this chapter introduces a general method
for fast MRI parameter estimation.

% signal models nonlinear
% so parameter estimation requires nonconvex optimization
% previous chapter described likelihood models
% several works [cite] have had success with this
% however these work for simple problems like single t1/t2 estimation
% for larger problems, undesirable or even intractable
% staroswiecki:12:seo 	fit m0,t1,t2,adc from dess im per-voxel
% ma:13:mrf		 					fit m0,t1,t2,b0 from mrf im per-voxel
% mcgivney:14:scf 	 		fit m0,t1,t2,b0 from low-rank mrf im using low-rank dict per-voxel
% zhao:14:mbm						recon m0,t2 from sse data via sparsity-constrained recon
% zhao:15:amp 					low-rank plus sparse recon w varpro for t1,t2 mapping
% beneliezer:15:raa 		fit m0,t2,b1 from mse im per-voxel
% zhao:16:mlr						recon m0,t1,t2 maps from mrf data via admm/varpro
% nataraj:17:oms				fit m0,t1,t2 from spgr/dess per-voxel
% asslander::lra 				similar to zhao:16:mlr, but claims better-conditioned initialization?
% cauley:15:fgm 				cluster full dict elements; use dict means to fit m0,t1,t2,b0 per-voxel
% doneva:17:mcb					est subspace of mrf k-t data from fully-sampled k-space center
% 											use subspace est for low-rank matrix completion of mrf data
% 											then recon matrix-completed mrf data and fit m0,t1,t2 per-voxel
% yang::lra 						fit m0,t1,t2,b0 from low-rank mrf im using coarse low-rank dict per-voxel
%												and then apply bilinear interpolation: x100 acceleration
Chapter~\ref{c,relax} applied
a common parameter estimation strategy to QMRI
that involves minimizing a cost function
related to a statistical likelihood function.
Because MR signal models are typically nonlinear functions
of the underlying latent parameters,
such likelihood-based estimation
usually requires non-convex optimization.
To seek good solutions,
many works
(\eg, 
\cite{%
	haldar:07:mle,%
	hernando:08:jeo,%
	barral:10:arm,%
	staroswiecki:12:seo,%
	ma:13:mrf,%
	trzasko:13:etf,%
	mcgivney:14:scf,%
	zhao:14:mbm,%
	beneliezer:15:raa,%
		zhao:15:amp,%
	cauley:15:fgm,%
	zhao:16:mlr,%
	nataraj:17:oms,%
	asslander::lra,%
	yang::lra%
})
approach estimation
with algorithms
that employ exhaustive grid search,
which requires either storing
or computing on-the-fly 
a ``dictionary'' of signal vectors.
These works estimate a small number (2-3)
of nonlinear latent parameters,
so grid search is practical.
However, 
for moderate or large sized problems,
the required number 
of dictionary elements
renders grid search undesirable or even intractable,
unless one imposes artificially restrictive latent parameter constraints.
Though several recent works
\cite{%
	mcgivney:14:scf,%
	cauley:15:fgm,%
	asslander::lra,%
	yang::lra%
}
focus on reducing dictionary storage requirements,
all of these methods ultimately rely 
on some form of dictionary-based grid search.

% clear need for a method that scales well with # parameters?
% multi-compartment: 6-11
% diffusion: at least 7
% phase-based methods: flow, b1, b0, asl? 
There are numerous QMRI applications
that could benefit from an alternative parameter estimation method
that scales well with the number of latent parameters.
For example,
vector (\eg, flow \cite{feinberg:85:mri})
and tensor 
(\eg, diffusivity \cite{bihan:01:dti} or conductivity \cite{tuch:01:ctm})
field mapping techniques
require estimation 
of at minimum 4 and 7 latent parameters per voxel,
respectively.
Phase-based longitudinal \cite{sekihara:85:nif} 
or transverse \cite{morrell:08:aps,sacolick:10:bmb} field mapping
could avoid noise-amplifying algebraic manipulations
on reconstructed image data
that are conventionally used
to reduce signal dependencies 
on nuisance latent parameters.
Compartmental fraction mapping \cite{mackay:94:ivv,nataraj:17:mwf}
from steady-state pulse sequences
requires estimation of at least 7 \cite{deoni:08:gmt}
and as many as 10 \cite{deoni:13:oct}
latent parameters per voxel.
In these and other applications,
greater estimation accuracy
requires more complete signal models
that involve more latent parameters,
increasing the need 
for scalable estimation methods.

% kernel methods
% fundamental challenge: nonlinear model
% classical idea: transform nonlinear problem into a linear one
% unclear how to balance complexity of transformation for accuracy with increase in dimensionality
% fortunately, simple transforms involving certain reproducing kernel functions yield solutions that need not scale in complexity with the dimensionality of the associated transformed data
% wahba introduced representation in approx theory
% scholkopf introduced in context of learning theory
% enjoyed success in many machine learning applications,
% originally for classification and later regression
The fundamental challenge 
of scalable MRI parameter estimation
stems from MR signal model nonlinearity:
standard linear estimators
would be scalable but inaccurate.
One natural solution strategy
involves nonlinearly preprocessing reconstructed images
such that the transformed images 
are at least approximately linear
in the latent parameters.
As an example,
for simple $\Tt$ estimation
from measurements at multiple echo times,
one could apply linear regression
to the logarithm of the measurements
(Section~\ref{s,demo} builds further intuition
using this simple application).
However,
such simple transformations
are generally not evident 
for more complicated signal models.
Without such problem-specific insight,
sufficiently rich nonlinear transformations
could dramatically increase problem dimensionality,
hindering scalability.
Fortunately, 
a celebrated result
in approximation theory \cite{kimeldorf:70:acb} showed
that simple transformations involving
\emph{reproducing kernel} functions \cite{aronszajn:50:tor}
can represent nonlinear estimators
whose evaluation need not directly scale in computation
with the (possibly very high) dimension
of the associated transformed data.
These kernel methods later found popularity
in machine learning
(initially for classification \cite{cortes:95:svn}
and quickly thereafter for other applications,
\eg, regression \cite{saunders:98:rrl})
because they provided simple, scalable nonlinear extensions
to fast linear algorithms.

% related work
The general idea
of using linearization
to simplify a nonlinear estimation problem
has been used before in QMRI.
For example,
orthogonal transforms
have been used
to linearly represent 
exponential \cite{huang:12:tmf}
and extended phase graph \cite{huang:13:trw} models
for $\Tt$ estimation.
An unscented Kalman filter 
has been used 
to linearly represent nonlinear models
for general multiple-parameter estimation
up to third-order accuracy \cite{zhao:16:daa}.
Whereas these prior works largely focus
on parameter estimation accuracy gains 
in under-sampled acquisitions,
this paper focuses on acceleration 
for general per-voxel MRI parameter estimation
from reconstructed images.

% idea here is to link parameter estimation 
% to a problem of regression
% ideas of machine learning can link estimation (seek parameter estimates from data assuming model) to regression (seek regression function relating inputs to outputs)
This chapter introduces  
a fast dictionary-free method
for MRI parameter estimation
via regression with kernels (PERK).
PERK first simulates many instances
of latent parameter inputs
and measurement outputs
using prior distributions
and a general nonlinear MR signal model.
PERK takes such input-output pairs
as simulated \emph{training points}
and then \emph{learns}
(using an appropriate nonlinear kernel function)
a nonlinear \emph{regression function}
from the training points.
PERK may scale considerably better
with the number of latent parameters
than likelihood-based estimation 
via grid search.

The remainder of this chapter
is organized as follows.
Section~\ref{s,perk,rev} reviews 
pertinent background information about kernels. 
Section~\ref{s,perk,meth} formulates 
a function optimization problem
for MRI parameter estimation
and efficiently solves this problem 
using kernels.
Section~\ref{s,perk,perf} studies bias and covariance
of the resulting PERK estimator.
Section~\ref{s,perk,pract} addresses practical implementation issues
such as computational complexity and model selection.
Section~\ref{s,perk,exp} demonstrates PERK
in numerical simulations
as well as phantom and \invivo experiments.
Section~\ref{s,perk,disc} discusses advantages,
challenges, and extensions.
Section~\ref{s,perk,conc} summarizes key contributions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
\label{s,perk,rev}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This brief section reviews
relevant definitions and facts about kernels.
A (real-valued) \emph{kernel} 
$k : \setQ^2 \mapsto \real$
is a function 
that describes a measure of similarity
between two pattern vectors 
$\bmq,\bmq' \in \setQ$.
The matrix $\bmK \in \reals{N \times N}$
associated with kernel $k$
and $N \in \setN$ patterns $\bmq_1,\dots,\bmq_N \in \setQ$
consists of entries
$k\paren{\bmq_n,\bmq_{n'}}$
for $n,n' \in \set{1,\dots,N}$.
A \emph{positive definite kernel} is a kernel
for which $\bmK$ is positive semidefinite (PSD)
for any finite set of pattern vectors,
in which case $\bmK$
is a \emph{Gram matrix}.
A \emph{symmetric kernel} satisfies 
$k\paren{\bmq,\bmq'} = k\paren{\bmq',\bmq}$
$\forall \bmq,\bmq' \in \setQ$.
We hereafter restrict attention
to symmetric, positive definite (SPD) kernels.

An SPD kernel $k : \setQ^2 \mapsto \real$
defines an inner product 
in a particular Hilbert function space $\rkhs$
that we briefly describe here
because it characterizes
the class of candidate regression functions
over which PERK operates.
To envision $\rkhs$,
first define a kernel's associated \emph{(canonical) feature map} 
$\bmz : \setQ \mapsto \reals{\setQ}$
that assigns each $\bmq \in \setQ$ 
to a \emph{(canonical) feature} $k\paren{\cdot,\bmq} \in \reals{\setQ}$.
Then $\rkhs$ is a completion 
of the space $\setH := \set{\sum_{n=1}^N a_n k\paren{\cdot,\bmq_n}}$
spanned by point evaluations
of the feature map,
where
$N \in \setN$,
$a_1,\dots,a_N \in \real$,
and
$\bmq_1,\dots,\bmq_N \in \setQ$ are arbitrary.
Let $\innprod{\cdot}{\cdot} : \rkhs^2 \mapsto \real$ 
denote the inner product on $\rkhs$.
Then for any $h,h' \in \setH$
that have finite-dimensional canonical representations
$h := \sum_{n=1}^N a_n k\paren{\cdot,\bmq_n}$ 
and
$h' := \sum_{n'=1}^N b_{n'} k\paren{\cdot,\bmq_{n'}}$,
the assignment
\begin{align}
	\innprod{h}{h'}_\rkhs =
		\sum_{n=1}^N \sum_{n'=1}^N a_n b_{n'} k\paren{\bmq_{n'},\bmq_n}
	\label{eq,inn-prod}
\end{align}
is consistent
with the inner product on $\rkhs$.
This inner product exhibits $\forall h\in\rkhs, \bmq\in\setQ$
an interesting \emph{reproducing property}
\begin{align}
	\innprod{h}{k\paren{\cdot,\bmq}}_\rkhs = h\paren{\bmq}
	\label{eq,rep-prop}
\end{align}
that can be seen to directly follow 
from \eqref{eq,inn-prod}
for $h \in \setH$.

A \emph{reproducing kernel} (RK) is a kernel 
that satisfies \eqref{eq,rep-prop}
for some real-valued Hilbert space $\rkhs$.
A kernel is reproducing if and only if it is SPD.
There is a bijection between RK $k$ and $\rkhs$,
and so $\rkhs$ is often called
the \emph{reproducing kernel Hilbert space} (RKHS)
uniquely associated with RK $k$.
This bijection is critical
to practical function optimization over an RKHS
in that it translates inner products 
in a (usually high-dimensional) RKHS $\rkhs$
into equivalent kernel operations 
in the (lower-dimensional) pattern vector space $\setQ$.
The following sections exploit 
the bijection between an RKHS 
and its associated RK.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Function Optimization Problem \& Kernel Solution}
\label{s,perk,meth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias and Covariance Analysis}
\label{s,perk,perf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Considerations}
\label{s,perk,pract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimentation}
\label{s,perk,exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{s,perk,disc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{s,perk,conc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
