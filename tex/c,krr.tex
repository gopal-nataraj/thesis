% kernel ridge regression for parameter estimation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{s,krr,intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In MRI \emph{parameter estimation},
one seeks to quantify biomarker ''maps''
(\ie, parameter images) 
from data.
Because MRI acquisitions 
are tunably sensitive
to many physical processes
(\eg, relaxation \cite{bloch:1946:ni-paper}, 
diffusion \cite{torrey:56:bew}, 
and chemical exchange \cite{mcconnell:58:rrb}),
MRI parameter estimation is important
in many QMRI applications
(\eg, relaxometry \cite{bloembergen:1948:rei}, 
diffusivity tensor imaging \cite{bihan:01:dti}, 
and multi-compartmental imaging \cite{mackay:94:ivv}). 
Motivated by such widespread applications,
this chapter describes a method
for fast MRI parameter estimation.

% signal models nonlinear
% so parameter estimation requires nonconvex optimization
% previous chapter described likelihood models
% several works [cite] have had success with this
% however these work for simple problems like single t1/t2 estimation
% for larger problems, undesirable or even intractable
Chapter~\ref{c,relax} applied
a common parameter estimation strategy
to QMRI
that involves minimization
of an objective function
related to the likelihood function
(and possibly regularization terms).
Because MR signal models are typically nonlinear functions
of latent object parameters,
such likelihood-based estimation
requires non-convex optimization in general.
To seek global optima,
several of our 
\cite{nataraj:14:rje,nataraj:14:mbe,nataraj::oms}
and others' 
\cite{staroswiecki:12:seo,ma:13:mrf,beneliezer:15:raa,zhao:16:mlr}
works approach estimation 
via exhaustive grid search,
which requires either storing
or computing on-the-fly a ``dictionary'' 
of scan profile signal vectors.
These works estimate a small (2-3)
number of latent parameters,
and so grid search is practical.
However, 
for even moderately sized problems,
the required number 
of dictionary elements
renders grid search undesirable or even intractable,
unless one assumes artificially restrictive latent parameter constraints.

% clear need for a method that scales well with # parameters?
% multi-compartment: 6-11
% diffusion: at least 7
% phase-based methods: flow, b1, b0, asl? 
There are numerous QMRI applications
that could benefit from an alternative MRI parameter estimation method
that scales well with the number of latent parameters.
For example,
vector (\eg, flow \cite{feinberg:85:mri})
and tensor 
(\eg, diffusivity \cite{bihan:01:dti} or conductivity \cite{tuch:01:ctm})
field mapping techniques
require estimation 
of at minimum 4 and 7 latent parameters per voxel,
respectively.
Phase-based longitudinal \cite{sekihara:85:nif} 
or transverse \cite{morrell:08:aps,sacolick:10:bmb} field mapping
could avoid noise-amplifying algebraic manipulations
on reconstructed image data
that are conventionally used
to reduce signal dependencies 
on nuisance latent parameters.
Compartmental fraction mapping \cite{mackay:94:ivv}
from steady-state pulse sequences
requires estimation of at least 7 \cite{deoni:08:gmt}
and as many as 10 \cite{deoni:13:oct}
latent parameters per voxel.
In these and other applications,
greater estimation accuracy
requires more complete signal models
that involve more latent parameters,
which only increases the need 
for scalable estimation methods.

% omit
% what has been done for larger problems?
% several works tried sim annealing, global opt, region contraction
% however, slow and have been shown to yield constraint-dependent estimates

% kernel methods
% fundamental challenge: nonlinear model
% kernel methods used to allow nonlinearity in classification, novelty detection, regression
% can relate estimation (where
The fundamental challenge 
of scalable MRI parameter estimation
stems from MR signal model nonlinearity:
standard linear estimators
would be scalable but inaccurate.
Opportunely,
so-called \emph{kernel functions} \cite{aronszajn:50:tor}
are often well-suited 
to transform nonlinear problems
into linear (albeit higher-dimensional) problems
that can be solved efficiently. 
Associated \emph{kernel methods} \cite{scholkopf:01:agr}
are quite general
and yield remarkably simple nonlinear extensions
to otherwise linear methods
for various machine learning problems,
\eg classification,
anomaly detection,
dimensionality reduction,
and regression.

% idea here is to link parameter estimation 
% to a problem of regression
% ideas of machine learning can link estimation (seek parameter estimates from data assuming model) to regression (seek regression function relating 
This chapter introduces a fast, scalable method 
for nonlinear MRI parameter estimation
via kernel ridge regression (KRR).
We observe that 
for voxel-wise separable MRI parameter estimation problems,
one can rapidly simulate many instances
of latent parameter inputs and signal outputs
from the nonlinear signal model.
We take such input-output pairs
as simulated \emph{training points}
and propose to then \emph{learn}
(using an appropriate kernel function)
a nonlinear \emph{regression function}
(\ie, non-iterative estimator)
from the training points.
The proposed KRR-based estimator scales considerably better
with the number of estimated latent parameters
than previously-discussed likelihood-based estimators.

This chapter is organized as follows.
Section~\ref{s,krr,meth}
reviews the general signal model
for an MR scan profile,
constructs and solves 
an appropriate functional optimization problem,
and further reduces the proposed estimator's
computational requirements
via a kernel approximation.
Section~\ref{s,krr,exp}
applies KRR-based estimation
to quantify six parameters arising 
from models describing the steady-state 
magnetization dynamics
of two water compartments,
a challenging application
of interest in myelin water fraction imaging 
(discussed in Chapter~\ref{c,mwf}).
Section~\ref{s,krr,summ}
discusses possible extensions 
and provides concluding remarks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Function Optimization Problem \& Kernel Solution}
\label{s,krr,meth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{A Functional Optimization Problem and Kernel Solution}
%\label{ss,krr,meth,opt}

Recall from Section~\ref{ss,relax,meth,prof}
that after image reconstruction,
many MRI acquisitions
produce at each voxel position
a sequence of noisy voxel values 
$\bmy \in \complexes{D}$, 
modeled as
\begin{align}
	\bmy = \bms\paren{\bmx; \bmnu} + \bmeps,
	\label{eq:krr,model}
\end{align}
where $\bmx \in \reals{L}$ denotes $L$ \emph{latent} object parameters 
(\eg, relaxation time constants);
$\bmnu \in \reals{K}$ denotes $K$ \emph{known} object parameters 
(\eg, separately acquired and estimated field maps);
$\bms : \reals{L} \times \reals{K} \mapsto \complexes{D}$ 
models the noiseless signals that arise from $D$ datasets;
and $\bmeps \in \complexes{D}$
is complex Gaussian noise,
assumed to be distributed as $\cgauss{\zeros{D}}{\bmSig}$.
Unlike signal models in earlier chapters
(\cf \eqref{eq:scn-dsgn,mod-vec-abbrev}),
Equation~\ref{eq:krr,model} omits
for simplicity
explicit model dependence on acquisition parameters,
as these are fixed 
during parameter estimation.

Here we seek to estimate 
on a per-voxel basis
each latent parameter $\bmx$
from corresponding data sequence $\bmy$ 
and known parameter $\bmnu$.
To develop (or train) a non-iterative estimator $\est{\bmx}$,
we simulate many instances 
of input-output relation \eqref{eq:krr,model}
and use kernels
to develop a nonlinear inverse relation.
We sample $\reals{L} \times \reals{K} \times \complexes{D}$
and evaluate \eqref{eq:krr,model} $N$ times
to produce sets of training inputs
$\set{\paren{\bmx_1,\bmnu_1,\bmeps_1},\dots,\paren{\bmx_N,\bmnu_N,\bmeps_N}}$
and data sequences
$\set{\bmy_1,\dots,\bmy_N}$. 
We seek a function
$\est{\bmh}~:~\reals{Q} \mapsto \reals{L}$
for $Q := 2D+K$
and an offset $\est{\bmb} \in \reals{L}$
that together map each pure-real regressor
$\bmq_n := [\re{\bmy_n}\tpose, \im{\bmy_n}\tpose, \bmnu_n\tpose]\tpose$,
$n \in \set{1,\dots,N}$, 
to an estimate 
$\bmxha{\bmq_n} := \bmhha{\bmq_n}+\est{\bmb}$ 
that is ``close'' to corresponding regressand $\bmx_n$:
\begin{align}
	\paren{\est{\bmh},\est{\bmb}} &\in 
		\set{\argmin{\substack{\bmh \in \hilb^L \\ \bmb \in \reals{L}}}
		\costa{\bmh, \bmb; \paren{\bmx_1,\bmq_1},\dots,\paren{\bmx_N,\bmq_N}}}, 
		\where \label{eq:krr,prob} \\
	\costa{\dots} &= 
		\sum_{l=1}^L \cost_l\paren{h_l,b_l; 
		\paren{x_{l,1},\bmq_1},\dots,\paren{x_{l,N},\bmq_N}}; 
		\label{eq:krr,cost} \\
	\Psi_l(\dots) &= 
		\frac{1}{N} \sum_{n=1}^N \paren{h_l(\bmq_n) + 
		b_l - x_{l,n}}^2 + \rho_l \norm{h_l}_\hilb^2.
	\label{eq:krr,cost-l}
\end{align}
Here, each $h_l~:~\reals{Q} \mapsto \real$ is a scalar function
that maps to the $l$th component of the output of $\bmh$; 
each $b_l,x_{l,n} \in \real$ are scalar components of $\bmb,\bmx$;
$\hilb$ is a (presently unspecified) Hilbert function space,
whose norm $\norm{\cdot}_\hilb$ is
induced by inner product 
$\innprod{\cdot}{\cdot}_\hilb : \hilb \times \hilb \mapsto \real$; 
each $\rho_l$ controls for regularity in $h_l$;
and $\paren{\cdot}\tpose$ denotes vector transpose.

Since \eqref{eq:krr,cost} is separable 
in the components of $\bmh$, 
it suffices to consider optimizing each $\paren{h_l,b_l}$ 
by separately minimizing \eqref{eq:krr,cost-l}
for each $l \in \set{1,\dots,L}$.
Nevertheless, 
minimizing \eqref{eq:krr,cost-l} 
is a challenging functional optimization problem
without further restriction 
on the Hilbert space $\hilb$.
Following many other works involving kernels,
we proceed by hereafter restricting $\hilb$
to be a \emph{reproducing kernel} Hilbert space 
(RKHS) \cite{aronszajn:50:tor}.
Remarkably,
a generalization
of the Representer Theorem \cite{scholkopf:01:agr},
restated as is relevant here for completeness,
then reduces \eqref{eq:krr,cost-l}
to a finite-dimensional optimization problem:
\begin{thm}[Generalized Representer, \cite{scholkopf:01:agr}]
	Define $k : \reals{Q} \times \reals{Q} \mapsto \real$
	to be the (symmetric positive definite) kernel function 
	associated with RKHS $\hilb$, 
	such that reproducing property $h_l(\bmq) = \innprod{h_l}{k(\cdot,\bmq)}_\hilb$
	holds for all $h_l \in \hilb$ and $\bmq \in \reals{Q}$. 
	Then any minimizer $(\est{h}_l,\est{b}_l)$ of \eqref{eq:krr,cost-l}
	over $\hilb \times \real$
	admits a representation of $\est{h}_l$ of the form
	\label{thm:rep}
	\begin{align}
		\est{h}_l(\cdot) \equiv \sum_{n=1}^N a_{l,n} k(\cdot,\bmq_{n}),
		\label{eq:krr,rep}
	\end{align}
	where each $a_{l,n} \in \real$ for $n \in \set{1,\dots,N}$.
\end{thm}

Theorem~\ref{thm:rep} ensures that any solution to 
\begin{align}
	(\est{\bma}_l,\est{b}_l) \in 
	\set{\argmin{\substack{\bma_l \in \reals{N} \\ b_l \in \real}} 
		\frac{1}{N} \sum_{n=1}^N \paren{\sum_{n'=1}^N a_{l,n'} k(\bmq_n,\bmq_{n'}) 
		+ b_l - x_{l,n}}^2 +
		\rho_l \norm{\sum_{n'=1}^N a_{l,n'} k(\cdot,\bmq_{n'})}^2_\hilb}
	\label{eq:krr,cvx}
\end{align}
corresponds via \eqref{eq:krr,rep} 
to a minimizer of \eqref{eq:krr,cost-l}
over $\hilb \times \real$,
where $\bma_l := [a_{l,1},\dots,a_{l,N}]\tpose$.
Fortunately, a solution of \eqref{eq:krr,cvx} exists uniquely
for $\rho_l > 0$
and can be expressed as
\begin{align}
	\est{\bma}_l &= \inv{\bmM \bmK + N\rho_l\eye{N}} \bmM \bmxg{l};
	\label{eq:krr,a-hat} \\
	\est{b}_l &= \frac{1}{N} \ones{N}\tpose \paren{\bmxg{l} - \bmK \est{\bma}_l},
	\label{eq:krr,b-hat}
\end{align}
where 
$\bmK \in \reals{N \times N}$ is the Gram matrix 
consisting of entries $k(\bmq_n,\bmq_{n'})$ for $n,n' \in \set{1,\dots,N}$;
$\bmM := \eye{N}-\frac{1}{N}\ones{N}\ones{N}\tpose$ is a de-meaning operator;
$\bmxg{l} := [x_{l,1},\dots,x_{l,N}]\tpose$;
$\eye{N} \in \reals{N \times N}$ is the identity matrix;
and $\ones{N} \in \reals{N}$ is a vector of ones.
In the special case where each
$\rho_l \gets \rho$ 
for fixed $\rho>0$, 
corresponding scalar estimators 
$\set{\est{x}_l\paren{\cdot} := \est{h}_l\paren{\cdot} + \est{b}_l}_1^L$
that arise from plugging \eqref{eq:krr,a-hat} into \eqref{eq:krr,rep}
can be concisely concatenated as
\begin{align}
	\bmxha{\cdot} &\gets \bmX 
		\paren{\frac{1}{N}\ones{N} + 
		\bmM\inv{\bmK\bmM + N\rho\eye{N}} \bmka{\cdot}},
		\label{eq:krr,x-hat}
\end{align}
where
$\bmka{\cdot} := 
\brac{k(\cdot,\bmq_1),\dots,k(\cdot,\bmq_N)}\tpose - \frac{1}{N}\bmK\ones{N}
: \reals{Q} \mapsto \reals{N}$
is a kernel embedding operator and 
$\bmX := \brac{\bmx_1,\dots,\bmx_N} = \brac{\bmxg{1},\dots,\bmxg{L}}\tpose 
\in \reals{L \times N}$
collects the regressands. 

% utility of estimate depends on kernel function
% valid kernel is q'*q, which corresponds to linear ridge regression
% more useful kernel is nonlinear fun of q; we use gaussian
For $\rho>0$, 
estimator \eqref{eq:krr,x-hat} minimizes \eqref{eq:krr,cost}
over $\paren{\hilb \times \real}^L$.
However, the utility of \eqref{eq:krr,x-hat}
depends on the choice of kernel $k$,
which induces a choice on the RKHS $\hilb$
and thus the function space $\paren{\hilb \times \real}^L$
over which \eqref{eq:krr,prob} optimizes.
For example, if $k$ was selected as the canonical dot product 
$k(\bmq,\bmq') \gets \innprod{\bmq}{\bmq'}_{\reals{Q}} := \bmq\tpose \bmq'$
(for which RKHS $\hilb \gets \reals{Q}$),
then \eqref{eq:krr,x-hat} would reduce 
to affine ridge regression \cite{hoerl:70:rrb},
which is optimal over $\paren{\reals{Q} \times \real}^L$
but is unlikely to be useful when signal model $\bms$ is nonlinear in $\bmx$.

Since we expect a useful estimate $\est{\bmx}\paren{\bmq}$ 
to depend nonlinearly (but smoothly) on $\bmq$,
we instead use a 
(symmetric, positive definite) 
$k$ that is likewise nonlinear in its arguments
and thus corresponds to a RKHS richer than $\reals{Q}$. 
Specifically, we use a Gaussian kernel
\begin{align}
	k(\bmq,\bmq') \gets \expa{-\frac{1}{2}\norm{\bmq-\bmq'}^2_{\bmL^{-2}}},
	\label{eq:krr,kern}
\end{align}
where symmetric, positive definite 
$\bmL \in \reals{Q \times Q}$ 
controls the length scales in $\bmq$ over which 
the estimator $\est{\bmx}$ smooths.
We focus on the Gaussian kernel
over other candidate kernels
because of its popularity
in the machine learning community 
for representing smooth functions.

Interestingly, 
the RKHS associated 
with Gaussian kernel \eqref{eq:krr,kern}
is infinite-dimensional.
Thus, 
Gaussian KRR
can be interpreted as 
first ``lifting'' 
via a nonlinear \emph{feature map} 
$\bmz : \reals{Q} \mapsto \hilb$ 
each $\bmq$ 
into an infinite-dimensional \emph{feature} $\bmza{\bmq} \in \hilb$,
and then performing affine ridge regression
on the features
via inner products of the form
$k(\bmq,\bmq') \gets \innprod{\bmza{\bmq}}{\bmza{\bmq'}}_{\hilb}$.
From this perspective,
the challenges of nonlinear estimation 
via likelihood models
are avoided 
because we \emph{select} 
(through the choice of kernel) 
characteristics of the nonlinear dependence
that we wish 
for our regression function to model
and need only \emph{estimate} via \eqref{eq:krr,cvx} 
the linear dependence
of $\est{\bmx}$ on the corresponding features.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Considerations}
\label{s,krr,pract,apprx}

This section focuses
on important practical implementation issues.
Subsection~\ref{ss,krr,pract,apprx} discusses
a scalable and conceptually intuitive approximation
of KRR estimator \eqref{eq:krr,x-hat}.
Subsection~\ref{ss,krr,pract,mod} describes guidelines
for nearly-automatic model selection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Kernel Approximation}
\label{ss,krr,pract,apprx}

In practical problems
with even moderately large ambient dimension $Q$,
the necessarily large number of training samples $N$ 
complicates storage of (dense) Gram matrix $\bmK$.
Using a kernel approximation 
can mitigate storage issues.
Here we choose
to sample Random (Fourier) Features \cite{rahimi:07:rff},
a recent method 
for approximating shift-invariants kernels
having form $k\paren{\bmq,\bmq'} \equiv k\paren{\bmq-\bmq'}$.
This subsection briefly reviews the main result of \cite{rahimi:07:rff}
for the purpose of constructing 
an intuitive and computationally efficient approximation of \eqref{eq:krr,x-hat}.

The strategy of \cite{rahimi:07:rff}
is to construct independent probability distributions 
$\dist{\bmv}$ and $\dist{s}$
associated with
random $\bmv \in \reals{Q}$ 
and random $s \in \real$ 
as well as a random function 
(that is parameterized by $\bmq$) 
$\zt\paren{\cdot,\cdot;\bmq} : \reals{Q} \times \real \times \reals{Q} \mapsto \real$,
such that
\begin{align}
	\expect{\bmv,s}{
		\zta{\bmv,s;\bmq}
		\zta{\bmv,s;\bmq'} 
	}
	= k(\bmq-\bmq'),
	\label{eq:krr,exp}
\end{align}
where 
$\expect{\bmv,s}{\cdot}$
denotes expectation with respect to $\dist{\bmv}\dist{s}$.
If such a construction exists,
one can build
approximate feature maps $\bmztZ$
by concatenating evaluations of 
$\ztZ:= \sqrt{2/Z}\zt$ on 
$Z$ samples 
$\set{\paren{\bmv_1,s_1},\dots,\bmv_Z,s_Z}$
of $\paren{\bmv,s}$
(drawn jointly albeit independently),
to produce approximate feature vectors
\begin{align}
	\bmztZa{\bmq} := \brac{\ztZa{\bmv_1,s_1;\bmq},\dots,\ztZa{\bmv_Z,s_Z;\bmq}}\tpose
	\label{eq:krr,feat}
\end{align}
for any $\bmq$. 
Then by the strong law of large numbers,
\begin{align}
	\lim_{Z \to \infty} \innprod{
		\bmztZa{\bmq}}{
		\bmztZa{\bmq'}
	}_{\reals{Z}} \overset{a.s.}{\to} k(\bmq,\bmq') \qquad \forall \bmq,\bmq',
	\label{eq:krr,lln}
\end{align}
which, 
in conjunction 
with strong performance guarantees
for finite $Z$ \cite{rahimi:07:rff}, 
justifies the interpretation 
of $\bmztZ$ as an approximate 
(and now finite-dimensional) feature map.

We use the Fourier construction 
of \cite{rahimi:07:rff}
that assigns
$\zt\paren{\bmv,s;\bmq} 
\gets 
\cosa{2\pi\paren{\bmv\tpose \bmq + s}}$.
If $s\sim\unif{0,1}$, 
then 
$\expect{\bmv,s}{
		\zt\paren{\bmv,s;\bmq}
		\zt\paren{\bmv,s;\bmq'} 
}$
simplifies to
\begin{align}
	\int_{\reals{Q}} \cosa{2\pi\bmv\tpose\paren{\bmq-\bmq'}} \dist{\bmv}\paren{\bmv} \der{\bmv}.
	\label{eq:krr,ft}
\end{align}
For symmetric positive definite $k$,
\eqref{eq:krr,ft} exists \cite{wu:97:gbt} 
and is the Fourier transform 
% with argument $\paren{\bmq-\bmq'}$
of $\dist{\bmv}$.
Thus, for Gaussian kernel \eqref{eq:krr,kern},
choosing $\bmv\sim\gauss{\boldsymbol{0}}{\paren{2\pi\bmL}^{-2}}$
satisfies \eqref{eq:krr,exp}.

Subsequent sampling of $\bmv,s$ 
and construction of 
$\bmZt := \brac{\bmztZa{\bmq_1},\dots,\bmztZa{\bmq_N}} \in \reals{Z \times N}$
via \eqref{eq:krr,feat}
gives for $Z \ll N$
a low-rank approximation $\bmZt\tpose\bmZt$ 
of Gram matrix $\bmK$.
Substituting this approximation into \eqref{eq:krr,x-hat}
and applying the matrix inversion lemma \cite{woodbury:50:imm} 
yields
\begin{align}
	\esta{\bmx}{\cdot} \gets \bmmx + \Cxzt\inv{\Cztzt + \rho\eye{Z}} \paren{\bmztZa{\cdot} - \bmmzt},
	\label{eq:krr,x-apx}
\end{align}
where $\bmmx := \frac{1}{N}\bmX\ones{N}$ 
and $\bmmzt := \frac{1}{N}\bmZt\ones{N}$ 
are sample mean vectors; and
$\Cxzt := \frac{1}{N}\bmX\bmM\bmZt\tpose$
and $\Cztzt := \frac{1}{N}\bmZt\bmM\bmZt\tpose$ 
are sample covariance matrices.
Estimator \eqref{eq:krr,x-apx} 
is a regularized variation
of the linear minimum mean-squared error estimator
on the features,
and illustrates
that Gaussian KRR via estimator \eqref{eq:krr,x-hat}
is asymptotically (in $Z$) equivalent
to affine ridge regression
after nonlinear, high-dimensional feature mapping.
%\todo{storage comparison with grid search}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model Selection}
\label{ss,krr,pract,mod}

This subsection proposes guidelines
for data-driven model selection
through a mixture
of intuitive arguments
and referenced analytical results.
Our goal here is to leave
for uninformed manual tuning
as few model parameters as possible.

\subsubsection{Choosing Sampling Distributions}
\label{sss,krr,pract,mod,dist}

To sample training points,
one must select prior distributions
on object parameters $\bmx,\bmnu$
and noise vector $\bmeps$. 
If offline training is required,
all of these distributions of course
must either be assumed
or empirically estimated
from previous experiments.
Here we focus on online training,
where one trains alongside data processing.

Latent object parameter distributions 
must account for model physics
(to focus samples on useful parameter ranges)
while also leaving generously heavy distribution tails
(to enable precise estimation 
over a broad parameter range).
Known object parameter distributions
can be estimated directly from known $\bmnu$
via density estimation methods, 
\eg \cite{parzen:62:oeo}.
Noise covariance is well-estimated
from low-signal regions of test data.

%\subsubsection{Choosing Number of Training Points $N$}
%\label{sss,krr,pract,mod,n}
% performance bounds (eg, pac statements) likely conservative
% empirical methods: bootstrap?

\subsubsection{Choosing Regularization Parameters}
\label{sss,krr,pract,mod,rho}

% krr from bayesian perspective
% assume x(q) = h(q)+b+eps is a noisy random function with Gaussian process prior
% eps is gaussian noise
% corresponding posterior mean is the krr estimate, with rho <- eps
% here we know selected noise variance, and can estimate cov(x) (when grad full col rank)
Regularization parameter selection is well guided
by considering KRR from an alternative Bayesian formulation.
In this perspective,
the unknown regression function
$\bmh\paren{\cdot}+\bmb$ arises randomly 
from a Gaussian process prior distribution
with mean function zero
and covariance function
given by the kernel.
The observed regressand model
$\bmx\paren{\bmq} = \bmh\paren{\bmq}+\bmb + \bmeps_\bmx$
includes additive \emph{latent parameter noise} $\bmeps_\bmx$ 
because noise is \emph{anticipated} in testing data
and is thus intentionally \emph{simulated} in training data.
If this latent parameter noise is assumed 
to be distributed as 
$\gauss{\zeros{L}}{N\diag{\brac{\rho_1,\dots,\rho_L}\tpose}}$,
then the mean function
of the regression function's posterior distribution
(after observation of training points)
happens to equal the KRR solution 
\eqref{eq:krr,a-hat}-\eqref{eq:krr,b-hat}
for each $l \in \set{1,\dots,L}$
(see \cite[Ch.~2]{rasmussen:05:gpf} for derivations).
In a Bayesian perspective then,
$N\rho_1,\dots,N\rho_L$ should reflect
latent parameter noise covariance.

In MRI parameter estimation,
we can more easily measure noise covariance
in image data $\bmSig$ 
than in latent parameters. 
We relate these two covariances
in the following.

A first-order Taylor expansion
of the signal model 
around $\expect{\bmx}{\bmx}$ is
\begin{align}
	\bmy \approx \expect{\bmy}{\bmy} + 
		\gradxs \paren{\bmx - \expect{\bmx}{\bmx}}.
	\label{eq:krr,model-approx}
\end{align}
Substituting \eqref{eq:krr,model-approx}
into a covariance definition gives
\begin{align}
	\bmSig &= 
		\expect{\bmy}{
			\paren{\bmy - \expect{\bmy}{\bmy}}
			\paren{\bmy - \expect{\bmy}{\bmy}}\ctpose
		}
		\nonumber \\
		&\approx \gradxs \cov{\bmx} \gradxs\ctpose.
		\label{eq:krr,cov-y}
\end{align}
Taking the Moore-Penrose pseudoinverse 
(denoted by $\pinv{\cdot}$)
of each side yields 
\begin{align}
	\gradxs\ctpose \inv{\bmSig} \gradxs &\approx 
		\label{eq:krr,messy} \\
	\gradxs\ctpose \pinv{\gradxs\ctpose} &\inv{\cov{\bmx}} \pinv{\gradxs} \gradxs.
		\nonumber
\end{align}
If the Jacobian matrix $\gradxs$ 
is of full column rank
(and is thus left-invertible), 
Eq.~\eqref{eq:krr,messy} simplifies
after matrix inversion to
\begin{align}
	\cov{\bmx; \bmnu} \approx \inv{\gradxs\ctpose \inv{\bmSig} \gradxs},
	\label{eq:krr,cov-x}
\end{align}
an expression reminiscent
of the \Cramer-Rao Bound 
(\cf Eq.~\eqref{eq:scn-dsgn,fisher}-\eqref{eq:scn-dsgn,crb}).

Expression~\eqref{eq:krr,cov-x} approximates
latent parameter noise covariance 
but is not yet useful
because it remains a function 
of known parameter $\bmnu$,
which varies spatially.
As a simple solution,
we set regularization parameters $\rho_1,\dots,\rho_L$
using diagonal elements of
$$\inv{N \gradxvs\ctpose \inv{\bmSig} \gradxvs},$$
where $\expx$ is taken
with respect to 
the distributions 
used to sample training points 
and $\nubar := \frac{1}{V} \sum_1^V \bmnu\paren{\bmr_v}$
is the sample mean 
of the known object parameters.

\subsubsection{Choosing Smoothing Length Scale}
\label{sss,krr,pract,mod,lam}

With online training,
it is intuitive
to set the Gaussian kernel's 
smoothing length scale 
based on test data sample means
$\bar{\bmy} := \frac{1}{V} \sum_{v=1}^V \bmy\paren{\bmr_v}$
and known parameter sample means $\nubar$:
\begin{align}
	\bmL \gets \lambda 
	\diag{\brac{\re{\bar{\bmy}}\tpose, \im{\bar{\bmy}}\tpose, \nubar\tpose}\tpose}.
	\label{eq:krr,length}
\end{align}
Here, $\lambda>0$ remains unspecified
as a unitless scalar free parameter
that balances training point consistency 
and estimator regularity.

\subsubsection{Choosing Kernel Approximation Order}
\label{sss,krr,pract,mod,h}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimentation}
\label{s,krr,exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Future Work}
\label{s,krr,summ}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% performance analysis: 
% how must $N$ scale with $L,Q$?
% storage comparison with grid search

% dealing with m0
% adding reconstruction
