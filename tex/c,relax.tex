% relaxometry

\section{Introduction}
\label{s,relax,intro}

This brief chapter describes methods
for QMRI parameter estimation
from statistical likelihood models.
The main purpose of this chapter
is to serve as a bridge 
between the background information
reviewed in Chapter~\ref{c,bkgrd}
and more novel ideas 
introduced in later chapters.
As such, 
we place emphasis here
on development of notation and terminology
over thorough validation.
As instructional examples,
we demonstrate likelihood-based parameter estimation
on simple problems
involving estimation 
of relaxation parameters $\To$ and $\Tt$,
applications
that Chapter~\ref{c,scn-dsgn} motivates and studies 
in much greater detail.

The remainder of this chapter 
is organized as follows.
Section~\ref{s,relax,meth}
introduces the notion of a QMRI scan profile,
describes a signal model for parameter estimation,
formulates several likelihood-based estimators
using this model,
and discusses practical implementation issues.
Section~\ref{s,relax,exp}
demonstrates the utility
of likelihood-based parameter estimation
over conventional estimators
through comparisons in simulation.
Section~\ref{s,relax,summ}
provides concluding remarks
and motivates the future work items
studied in subsequent chapters.

\section{Likelihood-Based Estimation in QMRI}
\label{s,relax,meth}
% signal model and construction of scan profile

\subsection{The QMRI Scan Profile}
\label{ss,relax,meth,prof}

After image reconstruction,
many MRI pulse sequences
useful for parameter estimation
produce at each voxel 
centered at position $\bmr$
a set of noisy voxel values 
$\set{y_1\pr,\dots,y_D\pr}$, 
each of which can be described
with the following general model:
\begin{align}
	y_d\pr = s_d\paren{\bmx\pr; \bmnu\pr, \bmp_d} + \epsilon_d\pr,
	\label{eq:relax,mod-scalar}
\end{align}
where 
$d \in \set{1,\dots,D}$. 
Here,
$\bmx\pr \in \complexes{L}$ 
collects $L$ \emph{latent} object parameters at $\bmr$;
$\bmnu\pr \in \complexes{K}$ 
collects $K$ \emph{known} object parameters at $\bmr$;
$s_d : \complexes{L} \times \complexes{K} \times \reals{A} \mapsto \complex$
is a (pulse-sequence dependent) function
that models the noiseless signal 
obtained from the $d$th dataset
using \emph{acquisition} parameter $\bmp_d \in \reals{A}$;
and $\epsilon_d \sim \cgauss{0}{\sigma_d^2}$ 
is assumed for simplicity 
\footnote{Though the noise distribution 
of $\bmk$-space raw data 
is usually well-modeled
as complex white Gaussian, 
the noise distribution 
of the $d$th reconstructed image $y_d$ depends both
on the acquisition and reconstruction.
If single receive channel $\bmk$-space data
is fully-sampled
on a Cartesian grid,
each dataset $y_d$ is recoverable
via separate Fourier transform,
and is thus complex Gaussian
and independent across datasets.
However if $\bmk$-space data
is multi-channel, undersampled, and/or Cartesian,
it may be preferable
that $y_d$ be estimated by more sophisticated techniques,
\eg \cite{fessler:03:nff, muckley:15:fpm}.
In such cases,
reconstructed image noise is unlikely
to be Gaussian-distributed.
}
to be (circularly-symmetric) complex Gaussian noise
\cite{macovski:96:nim, lei:07:som}
with zero mean and variance $\sigma_d^2$.
Semicolon positions 
in signal model \eqref{eq:relax,mod-scalar} 
and similar expressions throughout this thesis 
distinguish unknown and known parameters.
Concrete examples follow shortly.

For accurate, well-conditioned QMRI parameter estimation,
it is typically necessary 
to acquire a collection of datasets,
which we refer to hereafter as a \emph{scan profile}.
A scan profile consists 
of $D$ datasets
from up to $D$ pulse sequences
(some sequences yield more than one dataset, \eg DESS).
Let 
$\bmy\pr := \brac{y_1\pr, \dots, y_D\pr}\tpose \in \complexes{D}$
collect noisy voxel values centered at $\bmr$
from a given scan profile.
Then the vector signal model
\begin{align}
	\bmy\pr = \bms\paren{\bmx\pr; \bmnu\pr, \bmP} + \bmeps\pr
	\label{eq:relax,mod-vec}
\end{align}
helps define the noiseless signal 
$\bms := \brac{s_1, \dots, s_D}\tpose
: \complexes{L} \times \complexes{K} \times \reals{A \times D} \mapsto \complexes{D}$
and acquisition parameter
$\bmP := \brac{\bmp_1,\dots,\bmp_D} \in \reals{A \times D}$
associated with that scan profile.
Here, noise
$\bmeps\pr := \brac{\epsilon_1\pr, \dots, \epsilon_D\pr}\tpose \in \complexes{D}$
typically has diagonal covariance structure
$\bmSig := \diag{\brac{\sigma_1,\dots,\sigma_D}\tpose}$
due to independence across datasets,
where $\diag{\cdot}$ assigns its argument 
to the diagonal entries 
of an otherwise zero (square) matrix.

The following subsections
describe two concrete scan profiles
whose signals can be modeled 
via \eqref{eq:relax,mod-vec} 
and that we study through experiments
later in this chapter.

\subsubsection{Example: An SPGR Scan Profile for $\To$ estimation}
\label{sss,relax,meth,prof,t1}

We first consider the problem
of $\To\pr$ estimation at $\bmr$ 
from as few SPGR scans as possible,
given a prior estimate
of transmit field variation $\stx\pr$
(see \eqref{eq:flip-def}).
Examining SPGR model \eqref{eq:spgr-model}
makes clear that 
by fixing echo time $\TE$ across scans, 
SPGR signal dependence is reduced 
to just two spatially varying latent parameters:
desired parameter $\To\pr \in \real$ and 
nuisance parameter 
$\const{1}\pr := i\mzero\pr e^{-\TE/\Tts\pr} e^{-i\ompmed\pr \TE} \in \complex$.
We assign $\bmx \gets \brac{\To, \const{1}}\tpose$ and $\bmnu \gets \stx$
for $L \gets 2$ latent 
and $K \gets 1$ known parameters, respectively.

With $\TE$ fixed, 
prescribed flip angles $\flipnom$ 
and repetition times $\TR$ 
are the only remaining $A \gets 2$ 
acquisition parameters
available to choose
that appear explicitly in \eqref{eq:spgr-model}.
Thus, an SPGR scan profile 
useful for $\To$ estimation 
must vary 
$\bmp_d \gets \brac{\flipnom, \TR}\tpose
\forall d \in \set{1,\dots,D}$
over $\Ss$ scan repetitions
to produce $D \geq L \gets 2$ datasets
for well-conditioned estimation.

\subsubsection{Example: A DESS Scan Profile for $\Tt$ estimation}
\label{sss,relax,meth,prof,t2}

We next consider 
the problem of $\Tt\pr$ estimation at $\bmr$
from as few DESS scans as possible.
Examining DESS models 
\eqref{eq:dess-def-model} and \eqref{eq:dess-ref-model}
makes clear that even with fixed $\TE$
over possibly several acquisitions, 
there is signal dependence 
on five distinct object parameters:
$\stx\pr \in \real$,
$\To\pr \in \real$, 
$\ompmed\pr \in \real$,
$\const{2}\pr := \mzero\pr e^{-\TE/\Tts\pr} \in \complex$, 
and $\Tt\pr \in \real$.
In this chapter,
we take $\stx\pr \in \real$ and $\To\pr \in \real$
as known for simplicity.
To avoid (separate or joint) $\ompmed\pr$ estimation,
we choose to use magnitude DESS data,
at the expense of slight model mismatch
\footnote{The assumption of complex Gaussian noise 
in noisy MRI images
implies that corresponding magnitude MRI images
are Rician-distributed.
However,
the statistical estimators
we will develop
in Subsection~\ref{ss,relax,meth,est}
are based on Gaussian data.
Fortunately,
this source of model mismatch
is negligible (less than $1\%$)
for signal-to-noise ratio (SNR)
in excess of 10 \cite{gudbjartsson:95:trd},
and the acquisitions we examine here
are capable of producing SNR in tissue 
of at minimum $100$ and usually more. 
}
due to Rician noise.
These choices assign 
$\bmnu \gets \brac{\stx, \To}\tpose$ 
as $K \gets 2$
known parameters
and leave $L \gets 2$
latent parameters  
$\bmx \gets \brac{\Tt, \const{2}}\tpose$
to be estimated.

With $\TE$ again fixed, 
$\bmp_d \gets \brac{\flipnom, \TR}\tpose
\forall d \in \set{1,\dots,D}$
collects the remaining $A \gets 2$ 
tunable scan parameters
that appear explicitly in
\eqref{eq:dess-def-model} and \eqref{eq:dess-ref-model}.
As in Example~\ref{sss,relax,meth,sig,t1},
$D \geq L \gets 2$ datasets are necessary
for well-conditioned estimation.
Unlike before however,
a minimum $D \gets 2$ datasets 
need not require scan repetition,
since $\Sd$ DESS scan repetitions
produce $D \gets 2\Sd$ datasets.

\subsection{Latent Object Parameter Estimation}
\label{ss,relax,meth,est}

\subsubsection{Signal Model and Problem Statement}
\label{sss,relax,meth,est,sig}

A scan profile's reconstructed images
can be modeled 
to discretize the bulk MR signal 
into $V$ localized voxels
centered at positions $\bmr_1,\dots,\bmr_V$:
\begin{align}
	\bmY = \bmS\paren{\bmX; \bmN, \bmP} + \bmE.
	\label{eq:relax,mod-mtx}
\end{align}
Here, signal model
$\bmS : \complexes{L\times V} \times \complexes{K\times V}
\times \reals{A \times D} \mapsto \complexes{D \times V}$ 
is a matrix function
that maps latent 
$\bmX := \brac{\bmx\paren{\bmr_1},\dots,\bmx\paren{\bmr_V}} 
\in \complexes{L\times V}$
and known 
$\bmN := \brac{\bmnu\paren{\bmr_1},\dots,\bmnu\paren{\bmr_V}} 
\in \complexes{K\times V}$
parameter images 
(with fixed acquisition parameter $\bmP$)
to reconstructed image data
$\bmY := \brac{\bmy\paren{\bmr_1},\dots,\bmy\paren{\bmr_V}} 
\in \complexes{D\times V}$,
save for noise image
$\bmE := \brac{\bmeps\paren{\bmr_1},\dots,\bmeps\paren{\bmr_V}} 
\in \complexes{D\times V}$. 
The goal in QMRI parameter estimation
is to estimate latent parameter images $\bmX$ 
from MR image data $\bmY$,
for a fixed scan profile defined by $\bmS$ and $\bmP$
and given (separately acquired, estimated, and here assumed)
known parameter images $\bmN$.

\subsubsection{Maximum Likelihood Methods}
\label{sss,relax,meth,est,ml}
% from likelihood function to...
% ml cost

In maximum likelihood (ML) estimation,
one seeks to find model parameters
that maximize the likelihood
of observing output data.
We apply ML estimation to QMRI
by first constructing 
a \emph{likelihood function}
that describes the probability 
of observing image data $\bmY$
given latent parameters $\bmX$.
We then formulate 
ML latent parameter estimator $\estML{\bmX}$
by finding an estimate $\estaML{\bmX}{\bmY; \bmN, \bmP}$ 
of $\bmX$
that maximizes this likelihood function. 

We first construct the likelihood function
for the $v$th voxel's data $\bmy\paren{\bmr_v}$ 
and latent parameter $\bmx\paren{\bmr_v}$.
For complex Gaussian noise,
the likelihood function is
\begin{align}
	\Lf{\bmx\paren{\bmr_v}} \propto
	\expa{-\norm{\bmy\paren{\bmr_v}-
	\bms\paren{\bmx\paren{\bmr_v}; \bmnu\paren{\bmr_v}, \bmP}}^2_{\bmSig^{-1}}},
	\label{eq:relax,lf-vec}
\end{align}
where \eqref{eq:relax,lf-vec} omits constants
that are independent of $\bmx\paren{\bmr_v}$
and are therefore irrelevant.
Assuming noise independence 
across image voxels,
we can next build 
a simple and practical likelihood function
of the full image data as
\begin{align}
	\Lf{\bmX} &= \prod_{v=1}^V \Lf{\bmx\paren{\bmr_v}}.
	\label{eq:relax,lf-mtx}
\end{align}
We form an ML parameter estimate
by finding $\bmX$  
that maximizes this likelihood function:
\begin{align}
	\estaML{\bmX}{\bmY; \bmN, \bmP} &\in \set{\argmax{\bmX \in \setX^V} \Lf{\bmX}} 
	\nonumber \\
	&\equiv \set{\argmin{\bmX \in \setX^V} -\log{\Lf{\bmX}}}
	\label{eq:relax,ml-log-lf} \\
	&= \set{\argmin{\bmX \in \setX^V} 
	\sum_{v=1}^V \norm{\bmy\paren{\bmr_v} -
	\bms\paren{\bmx\paren{\bmr_v}; \bmnu\paren{\bmr_v}, \bmP}}^2_{\bmSig^{-1}}}
	\nonumber \\
	&= \set{\argmin{\bmX \in \setX^V} 
	\frob{\bmSig^{-1/2} \paren{\bmY - \bmS\paren{\bmX; \bmN, \bmP}}}^2},
	\label{eq:relax,ml-est}
\end{align}
where $\setX$ is a (typically convex) latent parameter search space;
the set equivalence in \eqref{eq:relax,ml-log-lf} 
uses the monotonicity of the $\log$ function;
and $\frob{\cdot}$ denotes the Frobenius matrix norm.

Typically, 
QMR image model $\bmS$ is nonlinear in $\bmX$
and so ML estimation problem \eqref{eq:relax,ml-est}
involves non-convex optimization,
which is challenging in general
(see Section~\ref{s,bkgrd,opt}).
Two properties 
of \eqref{eq:relax,ml-est}
guide our solution strategies.
First, 
\eqref{eq:relax,ml-est} is separable across voxels,
so problem non-convexity is addressable 
on a voxel-by-voxel basis.
Second,
MR signal models are usually partially linear,
in which case we may employ the variable projection method 
(described in Section~\ref{ss,bkgrd,opt,vpm})
to further reduce problem complexity.
For applications studied
in this chapter,
these properties allow for \eqref{eq:relax,ml-est}
to be solved via simple grid search.
 
\subsubsection{Regularized Likelihood Methods}
\label{sss,relax,meth,est,rls}
% rls cost 
% regularizers one might use

In regularized likelihood (RL) estimation,
we modify ML estimation problem \eqref{eq:relax,ml-log-lf}
to include additional information
in the form of \emph{regularization}:
\begin{align}
	\estaRL{\bmX}{\bmY; \bmN, \bmP} &\in
	\set{\argmin{\bmX \in \setX^V} -\log{\Lf{\bmX}} + \Rega{\bmX}}.
	\label{eq:relax,rl-est}
\end{align}
Here,
we have freedom to design regularizer 
$\Reg : \complexes{L \times V} \mapsto \real$ 
to encourage desirable structure
in estimates of $\bmX$. 
We observe
that it is usually reasonable
to assume that each latent object parameter map
is \emph{piecewise smooth} as a function of space:
that is, 
each parameter is likely 
to vary smoothly in space,
except for sharp discontinuities 
at tissue boundaries.
To encourage piecewise-smoothness 
in parameter estimates,
we use the regularizer 
\begin{align}
	\Rega{\bmX} &:= \sum_{l=1}^L \beta_l \sum_{j=1}^J
	\phi_l\paren{\brac{\bmJ\bmX\tpose}_{jl}}, \where
	\label{eq:relax,reg} \\
	\phi_l\paren{\cdot} &:= 
	\gamma_l^2 \paren{\sqrt{1 + \abs{\cdot/\gamma_l}^2} - 1}
\end{align}
is a differentiable approximation
of the absolute value function;
$\bmJ \in \reals{J\times V}$ 
evaluates $J$ (multi-dimensional) finite-differencing operations;
$\brac{\cdot}_{jl}$ extracts the $\paren{j,l}$th matrix element;
and $\beta_l$ is a regularization parameter
that controls the relative importance
of smoothing the $l$th latent object parameter image.
Conceptually,
this regularizer penalizes inconsistencies 
in adjacent latent parameter image voxels,
but with a severity that depends
on the degree of inconsistency. 
``Small'' voxel-to-voxel differences 
are likely due to image data noise
within a single tissue type
and are penalized near-quadratically, 
while ``large'' differences
are likely due to tissue boundaries
and are penalized near-linearly.
Useful notions 
of small versus large differences
are governed by shape parameters 
$\gamma_l\, \forall l\in\set{1,\dots,L}$,
and vary for different latent parameter maps
based on their units and relative scale.

In general, 
QMRI image signal model $\bmS$ is nonlinear in $\bmX$
and so RL estimation problem \eqref{eq:relax,rl-est}
requires non-convex optimization.
Unlike in ML estimation,
\eqref{eq:relax,rl-est} is not separable across voxels
due to regularization, 
precluding global optimization
(via grid search or other methods).
We instead take the corresponding ML estimate
as initialization
and solve \eqref{eq:relax,rl-est} 
via iterative constrained local optimization
(detailed in Section~\ref{ss,bkgrd,opt,loc}).

%\subsection{Practical Considerations}
%\label{ss,relax,meth,pract}
% preconditioner design

\section{Experimentation}
\label{s,relax,exp}
% experiments and results

This section demonstrates likelihood-based estimation
through two experiments in simulation
that correspond to the simple problems defined
in Section~\ref{ss,relax,meth,prof}.
Subsection~\ref{ss,relax,exp,t1} continues Example~\ref{sss,relax,meth,prof,t1}
in studying $\To$ estimation
from two SPGR scans.
Subsection~\ref{ss,relax,exp,t2} continues Example~\ref{sss,relax,meth,prof,t2}
in studying $\Tt$ estimation
from one DESS scan.

\subsection{$\To$ estimation from two SPGR scans}
\label{ss,relax,exp,t1}
% t1 estimation from spgr

We selected $\To$ and $\Tt$ WM and GM values
based on previously reported measurements at 3T 
\cite{wansapura:99:nrt, stanisz:05:ttr}
and extrapolated other nuisance latent object parameters
$\mzero$ and $\Tts$
from measurements at 1.5T \cite{kwan:99:msb}.
For simplicity,
we assumed no flip angle variation $\stx \gets 1$ 
and no phase accrual due to off-resonance effects $\ompmed \gets 0$.
We assigned these parameter values
to the 81st slice
of the BrainWeb digital phantom
\cite{collins:98:dac, kwan:99:msb}
to create ground truth $\bmMz, \bmTo, \bmTt, \bmTts \in \reals{V}$ maps.
We simulated $217 \times 181$
noiseless single-coil SPGR image datasets,
varying nominal flip angles $\flipnom \gets 5,30^{\circ}$
and fixing repetition times $\TR \gets 12.2$ms 
and echo times $\TE \gets 4.67$ms across $\Ss \gets 2$ scans.
We corrupted noiseless datasets
with additive complex Gaussian noise
to yield $D \gets 2$ noisy complex datasets
with signal-to-noise ratio (SNR) ranging from 105-122,
where SNR is defined here as
\begin{align}
	\snr{\bmS,\bmY} := \frac{\frob{\bmS}}{\frob{\bmY-\bmS}}.
	\label{eq:relax,snr}
\end{align}

We estimated latent parameter maps $\bmTo,\Const{1}$ 
using a conventional method-of-moments (MOM) estimator \cite{gupta:77:anl},
the ML estimator \eqref{eq:relax,ml-est}, 
and the RL estimator \eqref{eq:relax,rl-est}.
The MOM estimator applies linear regression voxel-by-voxel
to an appropriately transformed version
of the noiseless magnitude SPGR signal model
that is linear in $\To,\const{1}$;
see \eg \cite{gupta:77:anl,deoni:03:rct} for details.
We next describe our implementations 
of ML and RL estimation in turn.

The ML estimator applies the variable projection method
(VPM; described in Subsection~\ref{ss,bkgrd,opt,vpm})
to separate nonlinear $\bmTo$ estimation 
from linear $\Const{1}$ estimation.
Specifically,
the algorithm first estimates $\bmTo$ voxel-by-voxel
via an exhaustive grid search 
(over $1000$ $\To$ values
logarithmically spaced between $(10^{1.5}, 10^{3.5})$ms)
for a maximizer
of the separated least squares cost \eqref{eq:sep-ls-nonlin}.
It then estimates $\Const{1}$ 
via per-voxel linear regression \eqref{eq:sep-ls-fullrnk}.

The RL estimator applies a preconditioned variant
of the classical gradient projection method 
(GPM; described in Subsection~\ref{ss,bkgrd,opt,loc})
to iteratively descend 
towards a local optimizer 
of the RL cost
defined in \eqref{eq:relax,rl-est}.
We designed the preconditioner
as the inverse
of a positive definite diagonal majorizer
of the RL cost function's Hessian matrix,
updated for the first five iterations
and fixed thereafter.
We employed a diagonal preconditioner
to retain the linear convergence guarantees
of GPM \cite{bertsekas:82:pnm}
yet approach the practical performance
of other unprojected second-order methods
(\eg, Newton's method).
We employed a simple step-halving line search
at each iteration
to ensure monotone local convergence in cost.
We initialized GPM 
with the ML estimates.
We selected regularization parameters
as described in Subsection~\ref{sss,scn-dsgn,exp,phant,roi}.
We used the Michigan image reconstruction toolbox \cite{fessler:16:irt}
to construct the regularizer
and rapidly evaluate its gradient and Hessian.
We used the \matlab symbolic toolbox
to generate analytical expressions
for the gradient and Hessian 
of the SPGR signal model.
At each iteration,
we used these gradient and Hessian expressions
to compute a preconditioned descent direction,
update the iterate
(possibly after backtracking to ensure descent),
and project each voxel's $\To$ iterate
to within $\brac{10,3000}$ms.
We continued iterations
until the convergence criterion
\begin{align}
   \frob{\bmOmega^{-1}\paren{\iter{\bmX}{i}-\iter{\bmX}{i-1}}} 
   		< 10^{-7} \frob{\bmOmega^{-1}\paren{\iter{\bmX}{i}}}
   \label{eq:relax,conv-crit}
\end{align}
was satisfied,
where $\iter{\paren{\cdot}}{i}$ denotes the $i$th iterate,
$\bmOmega := \diag{\med{\iter{\bmX}{0}}}$
is a weighting matrix,
and $\med{\cdot}$ takes the median 
across the columns of its argument.

\begin{figure}[!t]
	\centering
	\subfigure{%
		\includegraphics [width=15.4cm] {%
			sp2de0,sl-81,t1,im,jet.eps%
		}
		\label{fig:relax,sim,t1,im,jet}
	}
	\hspace{0cm}
	\subfigure{%
		\includegraphics [width=15.2cm,trim=0 0 0 25,clip] {%
			sp2de0,sl-81,t1,err,jet.eps%
		}
		\label{fig:relax,sim,t1,err,jet}
	}
	\caption{%
		$\bmTo$ MOM, ML, and RL estimates
		and corresponding error images,
		from two simulated SPGR scans.
		Magnitude error images are $10\times$ magnified.
		Voxels not assigned WM- or GM-like relaxation times
		are masked out in post-processing for display.
		Table~\ref{tab:relax,sim,t1} 
		presents corresponding sample statistics.
	}
	\label{fig:relax,sim,t1}
\end{figure}

\begin{table}[!t]
	\small
	\centering
	\begin{tabular}{c | r | r r r}
		\hline
		\hline
										& Truth 	& MOM 										& ML 												& RL \\
		\hline
		WM $\To$ 				& $832$		& \mnstd{832.7}{15.6} 		& \mnstd{832.7}{15.6} 			& \mnstd{834.00}{2.77} \\
		GM $\To$ 				& $1331$ 	& \mnstd{1332}{34.9} 			& \mnstd{1332}{34.9} 				& \mnstd{1332.2}{6.3} \\
		\hline
		WM $\const{1}$ 	& $0.77$ 	& \mnstd{0.7266}{0.00744}	& \mnstd{0.7314}{0.00749} 	& \mnstd{0.73184}{0.00475} \\
		GM $\const{1}$ 	& $0.86$ 	& \mnstd{0.8245}{0.0108} 	& \mnstd{0.8301}{0.0109} 		& \mnstd{0.8287}{0.0059} \\
		\hline
		\hline
	\end{tabular}
	\caption{%
		Sample means $\pm$ sample standard deviations
		of MOM, ML, and RL $\To,\const{1}$ estimates
		from two simulated SPGR datasets,
		computed over $3001$ WM-like and $1151$ GM-like voxels.
		Each sample statistic is rounded off
		to the highest place value
		of its (unreported) standard error,
		computed via formulas in \cite{ahn:03:seo}.
		$\To$ values are in milliseconds.
		$\const{1}$ values are unitless.
		Fig.~\ref{fig:relax,sim,t1} 
		presents corresponding images.
	}
	\label{tab:relax,sim,t1}
\end{table}

Fig.~\ref{fig:relax,sim,t1} compares 
MOM, ML, and RL $\bmTo$ estimates
alongside $10\times$ magnified absolute difference images
with respect to the ground truth.
Overall, 
all three estimators produce reasonable $\bmTo$ maps.
The MOM and ML $\bmTo$ estimates are visually quite similar.
The RL $\bmTo$ estimate exhibits less variability 
well within WM and GM regions of interest (ROIs),
but incurs systematic bias along edges
and provides reduced spatial resolution.

Table~\ref{tab:relax,sim,t1} presents
$\To,\const{1}$ samples statistics
within WM-like and GM-like ROIs
selected to contain voxels
that are well away from tissue interfaces. 
In both WM and GM,
MOM and ML $\bmTo,\Const{1}$ estimates are comparable.
RL estimates consistently exhibit lower variability
but the RL $\bmTo$ estimate exhibits greater bias in WM.
All RL bias values would be significantly greater
if ROIs instead contained voxels at tissue interfaces.

\subsection{$\Tt$ estimation from one DESS scan}
\label{ss,relax,exp,t2}
% t2 estimation from dess


\section{Summary}
\label{s,relax,summ}

In the completed thesis,
this section will demonstrate likelihood-based estimation
in two simulation experiments
that correspond to the problems
defined in Examples~\ref{sss,relax,meth,sig,t1}-\ref{sss,relax,meth,sig,t2}.
The results will involve minor implementation improvements
to similar results presented 
in early conference papers 
\cite{nataraj:14:rje} and \cite{nataraj:14:mbe},
respectively.
The results will motivate our use 
in Chapter~\ref{c,scn-dsgn}
of likelihood-based estimation
over more conventional methods.

In the completed thesis,
this section will call into question
the somewhat arbitrary choices
of acquisition parameters 
used in the previous examples
and ask whether these acquisition parameters
can themselves be designed 
through optimization.
This question will serve 
as motivation for 
and as transition into Chapter~\ref{c,scn-dsgn}.
