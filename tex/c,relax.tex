% relaxometry

\section{Introduction}
\label{s,relax,intro}

This chapter introduces methods
for QMRI parameter estimation
from statistical likelihood models
and applies these methods
to simple problems 
in $\To$ and $\Tt$ relaxometry,
which are of interest
for monitoring the progression
of various disorders \cite{cheng:12:pma}.
Section~\ref{s,relax,meth}
introduces the notion of a QMRI scan profile,
describes a signal model for parameter estimation,
formulates several likelihood-based estimators
using this model,
and discuss practical implementation issues.
Section~\ref{s,relax,exp}
demonstrates the utility
of likelihood-based parameter estimation
over conventional methods
through simulation, phantom, and \invivo experiments.
Section~\ref{s,relax,summ}
provides brief concluding remarks
and suggests future directions.

\section{Likelihood-Based Estimation in QMRI}
\label{s,relax,meth}
% signal model and construction of scan profile

\subsection{The QMRI Scan Profile}
\label{ss,relax,meth,prof}

After image reconstruction,
many MRI pulse sequences
useful for parameter estimation
produce at each voxel 
centered at position $\bmr$
a set of noisy voxel values 
$\set{y_1\pr,\dots,y_D\pr}$, 
each of which can be described
with the following general model:
\begin{align}
	y_d\pr = s_d\paren{\bmx\pr; \bmnu\pr, \bmp_d} + \epsilon_d\pr,
	\label{eq:relax,mod-scalar}
\end{align}
where 
$d \in \set{1,\dots,D}$. 
Here,
$\bmx\pr \in \complexes{L}$ 
collects $L$ \emph{latent} object parameters at $\bmr$;
$\bmnu\pr \in \complexes{K}$ 
collects $K$ \emph{known} object parameters at $\bmr$;
$s_d : \complexes{L} \times \complexes{K} \times \reals{A} \mapsto \complex$
is a (pulse-sequence dependent) function
that models the noiseless signal 
obtained from the $d$th dataset
using \emph{acquisition} parameter $\bmp_d \in \reals{A}$;
and $\epsilon_d \sim \cgauss{0}{\sigma_d^2}$ 
is assumed for simplicity 
\footnote{Though the noise distribution 
of $\bmk$-space raw data 
is usually well-modeled
as complex white Gaussian, 
the noise distribution 
of the $d$th reconstructed image $y_d$ depends both
on the acquisition and reconstruction.
If single receive channel $\bmk$-space data
is fully-sampled
on a Cartesian grid,
each dataset $y_d$ is recoverable
via separate Fourier transform,
and is thus complex Gaussian
and independent across datasets.
However if $\bmk$-space data
is multi-channel, undersampled, and/or Cartesian,
it may be preferable
that $y_d$ be estimated by more sophisticated techniques,
\eg \cite{fessler:03:nff, muckley:15:fpm}.
In such cases,
reconstructed image noise is unlikely
to be Gaussian-distributed.
}
to be (circularly-symmetric) complex Gaussian noise
\cite{macovski:96:nim, lei:07:som}
with zero mean and variance $\sigma_d^2$.
Colon positions 
in signal model \eqref{eq:relax,mod-scalar} 
and similar expressions throughout this thesis 
distinguish unknown and known parameters.
Concrete examples follow shortly.

For accurate, well-conditioned QMRI parameter estimation,
it is typically necessary 
to acquire a collection of datasets,
which we refer to hereafter as a \emph{scan profile}.
A scan profile consists 
of $D$ datasets
from up to $D$ pulse sequences
(some sequences yield more than one dataset, \eg DESS).
Let 
$\bmy\pr := \brac{y_1\pr, \dots, y_D\pr}\tpose \in \complexes{D}$
collect noisy voxel values centered at $\bmr$
from a given scan profile.
Then the vector signal model
\begin{align}
	\bmy\pr = \bms\paren{\bmx\pr; \bmnu\pr, \bmP} + \bmeps\pr
	\label{eq:relax,mod-vec}
\end{align}
helps define the noiseless signal 
$\bms := \brac{s_1, \dots, s_D}\tpose
: \complexes{L} \times \complexes{K} \times \reals{A \times D} \mapsto \complexes{D}$
and acquisition parameter
$\bmP := \brac{\bmp_1,\dots,\bmp_D} \in \reals{A \times D}$
associated with that scan profile.
Here, noise
$\bmeps\pr := \brac{\epsilon_1\pr, \dots, \epsilon_D\pr}\tpose \in \complexes{D}$
typically has diagonal covariance structure
$\bmSig := \diag{\brac{\sigma_1,\dots,\sigma_D}\tpose}$
due to independence across datasets,
where $\diag{\cdot}$ assigns its argument 
to the diagonal entries 
of an otherwise zero (square) matrix.

The following subsections
describe two concrete scan profiles
whose signals can be modeled 
via \eqref{eq:relax,mod-vec} 
and that we study through experiments
later in this chapter.

\subsubsection{Example: An SPGR Scan Profile for $\To$ estimation}
\label{sss,relax,meth,sig,t1}

We first consider the problem
of $\To\pr$ estimation at $\bmr$ 
from as few SPGR scans as possible,
given a prior estimate
of transmit field variation $\stx\pr$
(see \eqref{eq:flip-def}).
Examining SPGR model \eqref{eq:spgr-model}
makes clear that 
by fixing echo time $\TE$ across scans, 
SPGR signal dependence is reduced 
to just two spatially varying latent parameters:
desired parameter $\To\pr \in \real$ and 
nuisance parameter 
$\const{1}\pr := i\mzero\pr e^{-\TE/\Tts\pr} e^{-i\ompmed\pr \TE} \in \complex$.
We assign $\bmx \gets \brac{\To, \const{1}}\tpose$ and $\bmnu \gets \stx$
for $L \gets 2$ latent 
and $K \gets 1$ known parameters, respectively.

With $\TE$ fixed, 
prescribed flip angles $\flipnom$ 
and repetition times $\TR$ 
are the only remaining $P \gets 2$ 
acquisition parameters
available to choose
that appear explicitly in \eqref{eq:spgr-model}.
Thus, an SPGR scan profile 
useful for $\To$ estimation 
must vary 
$\bmp_d \gets \brac{\flipnom, \TR}\tpose
\forall d \in \set{1,\dots,D}$
over $\Ss$ scan repetitions
to produce $D \geq L \gets 2$ datasets
for well-conditioned estimation.

\subsubsection{Example: A DESS Scan Profile for $\Tt$ estimation}
\label{sss,relax,meth,sig,t2}

We next consider 
the problem of $\Tt\pr$ estimation at $\bmr$
from as few DESS scans as possible.
Examining DESS models 
\eqref{eq:dess-def-model} and \eqref{eq:dess-ref-model}
makes clear that even with fixed $\TE$
over possibly several acquisitions, 
there is signal dependence 
on five distinct object parameters:
$\stx\pr \in \real$,
$\To\pr \in \real$, 
$\ompmed\pr \in \real$,
$\const{2}\pr := \mzero\pr e^{-\TE/\Tts\pr} \in \complex$, 
and $\Tt\pr \in \real$.
In this chapter,
we take $\stx\pr \in \real$ and $\To\pr \in \real$
as known for simplicity.
To avoid (separate or joint) $\ompmed\pr$ estimation,
we choose to use magnitude DESS data,
at the expense of slight model mismatch
\footnote{The assumption of complex Gaussian noise 
in noisy MRI images
implies that corresponding magnitude MRI images
are Rician-distributed.
However,
the statistical estimators
we will develop
in Subsection~\ref{ss,relax,meth,est}
are based on Gaussian data.
Fortunately,
this source of model mismatch
is negligible (less than $1\%$)
for signal-to-noise ratio (SNR)
in excess of 10 \cite{gudbjartsson:95:trd},
and the acquisitions we examine here
are capable of producing SNR in tissue 
of at minimum $100$ and usually more. 
}
due to Rician noise.
These choices assign 
$\bmnu \gets \brac{\stx, \To}\tpose$ 
as $K \gets 2$
known parameters
and leave $L \gets 2$
latent parameters  
$\bmx \gets \brac{\const{2}, \Tt}\tpose$
to be estimated.

With $\TE$ again fixed, 
$\bmp_d \gets \brac{\flipnom, \TR}\tpose
\forall d \in \set{1,\dots,D}$
collects the remaining $P \gets 2$ 
tunable scan parameters
that appear explicitly in
\eqref{eq:dess-def-model} and \eqref{eq:dess-ref-model}.
As in Example~\ref{sss,relax,meth,sig,t1},
$D \geq L \gets 2$ datasets are necessary
for well-conditioned estimation.
Unlike before however,
a minimum $D \gets 2$ datasets 
need not require scan repetition,
since $\Sd$ DESS scan repetitions
produce $D \gets 2\Sd$ datasets.

%\subsection{Signal Model for MRI Parameter Estimation}
%\label{ss,relax,meth,sig}
\subsection{Latent Object Parameter Estimation}
\label{ss,relax,meth,est}

\subsubsection{Signal Model and Problem Statement}
\label{sss,relax,meth,est,sig}

A scan profile's reconstructed images
can be modeled 
to discretize the bulk MR signal 
into $V$ localized voxels
centered at positions $\bmr_1,\dots,\bmr_V$:
\begin{align}
	\bmY = \bmS\paren{\bmX; \bmN, \bmP} + \bmE.
	\label{eq:relax,mod-mtx}
\end{align}
Here, signal model
$\bmS : \complexes{L\times V} \times \complexes{K\times V}
\times \reals{A \times D} \mapsto \complexes{D \times V}$ 
is a matrix function
that maps latent 
$\bmX := \brac{\bmx\paren{\bmr_1},\dots,\bmx\paren{\bmr_V}} 
\in \complexes{L\times V}$
and known 
$\bmN := \brac{\bmnu\paren{\bmr_1},\dots,\bmnu\paren{\bmr_V}} 
\in \complexes{L\times V}$
parameter images 
(with fixed acquisition parameter $\bmP$)
to reconstructed image data
$\bmY := \brac{\bmy\paren{\bmr_1},\dots,\bmy\paren{\bmr_V}} 
\in \complexes{D\times V}$,
save for noise image
$\bmE := \brac{\bmeps\paren{\bmr_1},\dots,\bmeps\paren{\bmr_V}} 
\in \complexes{D\times V}$. 
The goal in QMRI parameter estimation
is to estimate latent parameter images $\bmX$ 
from MR image data $\bmY$,
for a fixed scan profile defined by $\bmS$ and $\bmP$
and given (separately acquired, estimated, and here assumed)
known parameter images $\bmN$.

\subsubsection{Maximum Likelihood Methods}
\label{sss,relax,meth,est,ml}
% from likelihood function to...
% ml cost

In maximum likelihood (ML) estimation,
one seeks to find model parameters
that maximize the likelihood
of observing output data.
We apply ML estimation to QMRI
by first constructing 
a \emph{likelihood function}
that describes the probability 
of observing image data $\bmY$
given latent parameters $\bmX$.
We then formulate 
ML latent parameter estimate $\estaML{\bmX}{\bmY; \bmN, \bmP}$ 
by finding an $\bmX$
that maximizes this likelihood function. 

We first construct the likelihood function
for the $v$th voxel's data $\bmy\paren{\bmr_v}$ 
and latent parameter $\bmx\paren{\bmr_v}$.
For complex Gaussian noise,
the likelihood function is
\begin{align}
	\Lf{\bmx\paren{\bmr_v}} \propto
	\expa{-\norm{\bmy\paren{\bmr_v}-
	\bms\paren{\bmx\paren{\bmr_v}; \bmnu\paren{\bmr_v}, \bmP}}^2_{\bmSig^{-1}}},
	\label{eq:relax,lf-vec}
\end{align}
where \eqref{eq:relax,lf-vec} omits constants
that are independent of $\bmx\paren{\bmr_v}$
and are therefore irrelevant.
Assuming noise independence 
across image voxels,
we can next build 
a simple and practical likelihood function
of the full image data as
\begin{align}
	\Lf{\bmX} &= \prod_{v=1}^V \Lf{\bmx\paren{\bmr_v}}.
	\label{eq:relax,lf-mtx}
\end{align}
We form an ML parameter estimate
by finding $\bmX$  
that maximizes this likelihood function:
\begin{align}
	\estaML{\bmX}{\bmY; \bmN, \bmP} &\in \set{\argmax{\bmX \in \setX^V} \Lf{\bmX}} 
	\nonumber \\
	&\equiv \set{\argmin{\bmX \in \setX^V} -\log{\Lf{\bmX}}}
	\label{eq:relax,ml-log-lf} \\
	&= \set{\argmin{\bmX \in \setX^V} 
	\sum_{v=1}^V \norm{\bmy\paren{\bmr_v} -
	\bms\paren{\bmx\paren{\bmr_v}; \bmnu\paren{\bmr_v}, \bmP}}^2_{\bmSig^{-1}}}
	\nonumber \\
	&= \set{\argmin{\bmX \in \setX^V} 
	\frob{\bmSig^{-1/2} \paren{\bmY - \bmS\paren{\bmX; \bmN, \bmP}}}^2},
	\label{eq:relax,ml-est}
\end{align}
where $\setX$ is a (typically convex) latent parameter search space;
the set equivalence in \eqref{eq:relax,ml-log-lf} 
uses the monotonicity of the $\log$ function;
and $\frob{\cdot}$ denotes the Frobenius matrix norm.

Typically, 
QMR image model $\bmS$ is nonlinear in $\bmX$
and so ML estimation problem \eqref{eq:relax,ml-est}
involves non-convex optimization,
which is challenging in general
(see Section~\ref{s,bkgrd,opt}).
Two properties 
of \eqref{eq:relax,ml-est}
guide our solution strategies.
First, 
\eqref{eq:relax,ml-est} is separable across voxels,
so problem non-convexity is addressable 
on a voxel-by-voxel basis.
Second,
MR signal models are usually partially linear,
in which case we may employ the variable projection method 
(described in Section~\ref{ss,bkgrd,opt,vpm})
to further reduce problem complexity.
For applications studied
in this chapter,
these properties allow for \eqref{eq:relax,ml-est}
to be solved via simple grid search.
 
\subsubsection{Regularized Likelihood Methods}
\label{sss,relax,meth,est,rls}
% rls cost 
% regularizers one might use

In regularized likelihood (RL) estimation,
we modify ML estimation problem \eqref{eq:relax,ml-log-lf}
to include additional information
in the form of \emph{regularization}:
\begin{align}
	\estaRL{\bmX}{\bmY; \bmN, \bmP} &\in
	\set{\argmin{\bmX \in \setX^V} -\log{\Lf{\bmX}} + \Rega{\bmX}}.
	\label{eq:relax,rl-est}
\end{align}
Here,
we have freedom to design regularizer 
$\Reg : \complexes{L \times V} \mapsto \real$ 
to encourage desirable structure
in estimates of $\bmX$. 
We observe
that it is usually reasonable
to assume that each latent object parameter map
is \emph{piecewise smooth} as a function of space:
that is, 
each parameter is likely 
to vary smoothly in space,
except for sharp discontinuities 
at tissue boundaries.
To encourage piecewise-smoothness 
in parameter estimates,
we use the regularizer 
\begin{align}
	\Rega{\bmX} &:= \sum_{l=1}^L \beta_l \sum_{j=1}^J
	\phi_l\paren{\brac{\bmJ\bmX\tpose}_{jl}}, \where
	\label{eq:relax,reg} \\
	\phi_l\paren{\cdot} &:= 
	\gamma_l^2 \paren{\sqrt{1 + \abs{\cdot/\gamma_l}^2} - 1}
\end{align}
is a differentiable approximation
%(with shape parameter $\gamma_l$)
the absolute value function;
$\bmJ \in \reals{J\times V}$ 
evaluates $J$ (multi-dimensional) finite-differencing operations;
$\brac{\cdot}_{jl}$ extracts the $\paren{j,l}$th matrix element;
and $\beta_l$ is a regularization parameter
that controls the relative importance
of smoothing the $l$th latent object parameter image.
Conceptually,
this regularizer penalizes inconsistencies 
in adjacent latent parameter image voxels,
but with a severity that depends
on the degree of inconsistency. 
``Small'' voxel-to-voxel differences 
are likely due to image data noise
within a single tissue type
and are penalized near-quadratically, 
while ``large'' differences
are likely due to tissue boundaries
and are penalized near-linearly.
Useful notions 
of small versus large differences
are governed by shape parameters 
$\gamma_l\, \forall l\in\set{1,\dots,L}$,
and vary for different latent parameter maps
based on their units and relative scale.

In general, 
QMRI image signal model $\bmS$ is nonlinear in $\bmX$
and so RL estimation problem \eqref{eq:relax,rl-est}
requries non-convex optimization.
Unlike in ML estimation,
\eqref{eq:relax,rl-est} is not separable across voxels
due to regularization, 
precluding global optimization
(via grid search or other methods).
We instead take the corresponding ML estimate
as initialization
and solve \eqref{eq:relax,rl-est} 
via iterative constrained local optimization
(detailed in Section~\ref{ss,bkgrd,opt,loc}).

%\subsection{Practical Considerations}
%\label{ss,relax,meth,pract}
% preconditioner design

\section{Experimentation}
\label{s,relax,exp}
% experiments and results
% t1 estimation from spgr
% t2 estimation from dess

\section{Summary}
\label{s,relax,summ}
