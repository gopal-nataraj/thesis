script for talk

I'm sure many of you are quite familiar 
with quantitative mri,
but I figured I'd give a one-line overview
for the uninitiated. 
Broadly,
the goal of quantitative mri
is to rapidly and accurately localize biomarkers
from MR data.

Here,
a biomarker is a measurable tissue property
like elasticity
that characterizes some biological process
of interest
like sclerosis,
or the abnormal hardening of tissue.

By localize,
we mean that seek to quantify these biomarkers
at discretized positions in space.

By accurately,
we mean that we need to use signal models
that describe the underlying physics 
in sufficient detail.

By rapidly,
we mean that we would like
to use both fast MR acquisitions
and fast estimation methods.

A key challenge of QMRI
is that 'rapidly' and 'accurately'
are often competing goals:
more accurate models typically depend 
on both more biomarkers and nuisance markers,
but estimating more markers usually requires
longer scans and more computation.
I'll be speaking today
about some tools we recently developed 
that help address these challenges.


Specifically,
I will be describing new answers
to two questions in this talk.
First,
how can we systematically assemble
fast, informative collections of scans
to enable precise biomarker quantification?

Second,
given data from an informative acquisition,
how can we rapidly quantify these biomarkers?

At the end of the talk,
I'll discuss our ongoing efforts
to apply these tools
for myelin imaging.

I will begin
with acquisition design.


After reconstruction,
each voxel within a conventional mr image
can be described
with this general nonlinear signal model.
Here,
s_d relates L unknown parameters x,
K known parameters nu,
and A acquisitions parameters p_d
to a single voxel y_d of the dth image,
barring complex-gaussian noise epsilon_d.

A scan profile contains capital D such measurements,
and is characterized
by a general vector signal model, bold s.
For example,
in t2 mapping
x might denote t2,
nu might denote a separately estimated b1 map,
y might denote a vector of measurements
at different echo times,
and
P might denote the echo times.

In acquisition design,
the task is to design acquisition parameter matrix P
to enable precise unbiased esitmation
of biomarkers of interest,
which are some subset
of the elements of x.


To build an appropriate objective function,
we use the Fisher information matrix
to relate unbiased estimation precision
to the acquisition parameter matrix.
For complex Gaussian data,
the Fisher information can be interpretted 
to mean that the signal provides high information in x
for acquisition parameters P
that yield a large signal gradient.

When the Fisher matrix exists,
the cramer-rao bound ensures
that the covariance of unbiased estimate x-hat
is bounded below
by the inverse of the Fisher matrix,
where this matrix inequality means
that the difference matrix
is positive semi-definite.

This inequality is asympotically tight
for maximum-likelihood estimators,
so for sufficiently high snr data,
the inverse fisher matrix
is a good proxy
for unbiased estimator covariance.

The idea then is to choose acquisition parameters P
such that the inverse fisher matrix
is 'small' in some matrix sense.


More concretely,
we seek P that minimizes an objective function
that is a weighted sum
of the diagonal entries
of the inverse Fisher matrix,
which characterize variances
of scalar entries of x.

This objective cannot be optimized directly
because of its dependence
on object parameters x and nu,
which vary spatially.

Instead,
we consider two alternate problems.
In min-max scan design,
we seek candidate scan parameters P-cup
that minimize the worst-case imprecision,
viewed over tight object parameter ranges set-X and set-N.

In Bayesian scan design,
we instead seek candidate scan parameters
that minimize the expected imprecision,
where the expectation is with respect to
some prior joint distribution on x and nu.
Comparing the two,
min-max design
takes milder distributional assumptions
but involves an objective
that is non-differentiable in P.

We study here the min-max design criterion
and will later return
to the Bayesian criterion
for a more challenging application.


As a demonstration of min-max scan design,
we design a fast acquisition
for precise estimation
of relaxation parameters t1,t2
in white matter and gray matter
of the human brain at 3-tesla field strength.

Specifically,
we consider scan profiles consisting
of spoiled gradient-recalled echo (spgr)
and dual-echo steady-state (dess),
two fast steady-state MR pulse sequeneces.

We choose a time constraint
that allows two spgr scans and one dess scan,
a somewhat conventional t1/t2 acquisition,
and then optimize all scan profiles
that are feasible under this time constraint.
Observe that we take flip angle variation
to be separately estimated and here assumed known.
Observe also
that we take weighting matrix W
to place no emphasis on scale factor m0 estimation
and to place roughly equal emphasis on t1,t2 estimation.


Under the previously mentioned time constraint,
three scan profiles are both feasible
and produce at least as many datasets as latent variables,
each respectively consisting
of (2,1), (1,1), or (0,2) spgr/dess scans.
Here,
we summarize their optimized flip angles,
repetition times,
and optimized cost function values.

Our main finding is
that under this time constraint,
2 optimized dess sequences alone
can produce t1,t2 estimates at least as precise
as spgr/dess scan profiles.


We compared our optimized acquisitions
through phantom experiments.
Over a 256x256x8 fully-sampled 3d matrix,
each of these three fast profiles
took less than 2 minutes.

To assess accuracy in vivo,
we also collected a slower reference scan profile
consisting of 4 inversion recovery
and 4 spin echo single-slice acquisitions.

Lastly,
we collected 2 Bloch-Siegert SPGR scans
for separate flip angle calibration.


We first report phantom accuracy results.
Here,
we plot sample means and sample standard deviations
pooled over 100s of voxels
within 14 regions of interest.
We compare t1/t2 ml estimates
from the three candidate and one reference profile,
versus nist nmr measurements.
The orange region highlights
vials within the 'tight' parameter range.
Within this regions,
t1,t2 estimates from all profiles
exhibit minimal bias.


We next report phantom precision results.
Here,
we repeated each profile 10 times
and estimated t1,t2 std dev
of typical voxels within each vial
across the repetitions.

Pooling these sample standard deviation estimates
within the orange-marked vials,
we can assess the performance
of min-max scan design
by examining the empirical worst-case precision
across scan profiles.

Comparing against the optimized costs,
we observe similar trends across profiles
of empirical vs. predicted standard deviations.


In summary,
we have introduced an MR scan design method
to enable precise parameter estimation,
and we have demonstrated the method
by designing three SPGR/DESS scan profiles
for t1,t2 estimation in the brain.

Phantom and simulation results validated the method
as predictive of unbiased estimation precision.
We did not attempt 
to replicate these precision experiments in vivo
because of motion considerations,
but we did assess accuracy.

Here, 
columns denote the three candidate profiles
and the slow reference profile;
rows denote t1 vs t2 estimates.
We used narrow colorbars
to distinguish the wm and gm boundary.
Overall,
the wm/gm boundaries are similarly distinguishable 
across the t1 estimate
and the (1,1) t2 estimates exhibit much higher wm variation
than the other profiles,
as expected due to low relative precision.

However,
there are significant discrepencies across the profiles
(especially in t2 estimates),
which is suggestive of multi-compartmental relaxation.

To address this type of model mismatch,
we will need to develop more complete in vivo signal models
and will need scalably estimate more parameters,


which brings me to the second portion of this talk.


Here,
we continue to use a general signal model,
except we now omit explicit dependence
on scan parameter matrix P,
as it is fixed during parameter estimation.

Our task then is to estimate
latent parameter x,
given image data y and known parameter nu,
on a voxel-by-voxel basis.


this can be a challenging problem
because as mentioned before
s is often a nonlinear function of x,
so inverse problems
based on standard likelihood functions
are non-convex.
furthermore,
signal s might be difficult
to write down analytically.

In some simpler applications
where good initializations
and signal gradients are available,
gradient-based local optimization
may be possible.

In more challenging applications,
researchers have tried
stochastic methods like simulated annealing.

In general though,
the most reliable and popular method
is based on discretizing the parameters
over a grid of possible values
and exhaustively searching
for the parameter
that produces the best fit
with the image data.
In fact,
we employed this method
in the first section.


but let's take a look
at the computational cost of grid search.
In simple t1,t2 estimation
(like in fingerprinting)
there are 3 latent parameters.
discretizing over the 2 nonlinear parameters,
we need about 100 squared or 10,000 dictionary atoms,
which is manageable.  
But let's recall 
some of the more challenging
applications mentioned before.
These can have 
4,7 and even up to 10 latent parameters,
and the corresponding numbers
of dictionary atoms quickly becomes unmanageable.

So our goal is
to find a method
that scales with L more gracefully.


We approach this problem
taking inspiration from machine learning.
The idea is to learn a nonlinear estimator
from simulated training data.

Specifically,
we sample N instances
of latent and known parameters
and noises
and simulate corresponding image data vectors.

We would like to then construct
nonlinear functions h_l and offsets b_l
that in some sense ''inverts'' the signal model
by mapping image data vectors and known parameters
back to the lth latent parameter

Mathematically,
we would like to solve
this function optimization problem

but cannot because it is ill-posed:
there are infinitely many functions
that fit a finite N training points.

So we instead modify the problem
to restrict the function space
over which we optimize
and encourage regularity in the estimator
through function regularization.
It so happens
that the optimal estimator can be expressed
as a linear combination 
of so-called kernel functions,
so solving the function optimization problem
is equivalent to solving
for the kernel weights and the offset,
which is a standard convex optimization problem
that we can solve exactly.


When the dust settles,
the solution can be written in one line.
Here bold x_l collects the training point regressands,

bold uppercase K is the NxN Gram matrix,

M is a demeaning operator
and itself introduces identity matrix In
and the Nx1 ones vector,

and bold lowercase k is a nonlinear kernel embedding operator.

So let us return to the original goal:
does this PERK estimator scale better with L?
Apparently it does,
since by construction the estimators
are constructed separably!

However,
in practice more complex problems
will likely require more training samples N,
for which it is undesirable
to explicitly compute, store, and/or invert
NxN Gram matrix K.
Fortunately,
for many useful kernels
there exist very accurate kernel approximations,
which as a plus can provide us
some intuition about the PERK solution.


Suppose there exists a function z-tilde
of moderate dimension capital Z
such the Gram matrix admits this low-rank decomposition.
Here capital Z should be larger
than the dimension of the input space
(so as to still lift the inputs into higher dimension).

Plugging this low-rank decomposition
into the PERK solution
gives what appears to be a messy expression,
but recognizing some terms
as means and covariances,

we see that PERK appears (at least approximately)
to perform regular linear regression
but in a dimension higher
than the dimension of the input space!

But does such an approximate feature mapping
exist and work well?
Yes, at least for certain shift-invariant PSD kernels
like the Gaussian.
In this case,
if at least in practice
the product 'NZ' can be scaled less
than exponentially with L,
we have improved scalability.


We demonstrated PERK for t1,t2 estimation
from one of the previously optimized scan profiles.

To ensure we were learning a well-conditioned estimator,
we trained PERK using many samples drawn 
from a prior distribution on object parameters x, nu
whose support was chosen to coincide
with the support over which we performed min-max scan design.

We then compared PERK estimates 
to grid search estimates.


Here we compare PERK and grid search estimates in phantom.
The yellow boxes denote projections 
of the sampling distribution's support
over which PERK was trained.

We observe that within this support,
PERK and grid search agree excellently.


We then compared PERK and grid search 
for in vivo t1 and t2 estimation
in a single slice of the optimized acquisition.
The estimates agree reasonably in WM and GM,

but PERK is about 23x faster, 
including training time.
Note that only PERK testing time 
scales with the number of voxels,
so this acceleration factor would increase
for a typical 3D problem at this resolution.


In summary,
we recently introduced PERK,
a fast, dictionary-free machine-learning inspired method
for QMRI parameter estimation.

We demonstrated PERK in a simple, easily-validated problem
in which it was consistently 23x faster 
than dictionary-based grid search.

Recently however,
we have been interested
in whether we can exploit PERK's speed
for harder problems,


which brings me
to our ongoing work to use these tools
for myelin water fraction imaging.


Myelin is a lipid-rich substance
that in normal white matter wraps healthy axons,
thereby forming an electrically insulating layer
like rubber around a copper wire.
There is water trapped between these myelin layers,
and when myelin is damaged 
in demyelinating diseases like multiple sclerosis,
this water is released 
into the surrounding.

Myelin water fraction
denotes the proportion of mr signal
that arises from water trapped within the myelin layers
relative to the total water signal,

and has been shown
to correlate well with myelin content.


The gold-standard MWF imaging acquisition remains
a multi-echo spin-echo sequence,
from which MWF is characterized 
using a multi-exponential model
of the echo train decay.
However,
these experiments typically use long repetition times,
likely at least in part
to limit multi-compartmental t1 recovery effects.

More recently,
combinations of fast steady-state scans
using variable flip angles
(mcDESPOT) 
were shown to produce whole-brain mwf images
in about a half-hour of imaging.
However,
myelin water fraction estimates
from mcDESPOT were shown to disagree with MESE estimates,
likely due to insufficient estimation precision.

So our ongoing goal here 
is to design an imaging workflow
that enables fast, accurate MWF quantification in WM.


Here is a simple voxel-scale model
of how myelin water influences mr signal.
Specifically,
we now distinguish signal
to arise from a 'fast'-relaxing
and a 'slow'-relaxing water compartment.

The physics of these compartments
can be described with six free parameters.

Of these,
we are interested in estimating the fast-relaxing fraction ff,
a proxy for myelin water fraction
and a potential biomarker
for certain wm diseases.


Since our previous single-compartment t1,t2 estimates
from spgr/dess scan profiles
demonstrated sensitivity to multi-compartmental relaxation,
we found it natural to use
2-compartment spgr/dess models.

We studied the assumptions 
of a previously developed 2-compartment spgr model

and found
that their absorption of off-resonance effects
into m0 implies either assuming
that different compartments
have the same off-resonance broadening distribution
which may not be physically accurate,
or neglecting exchange
between excitation and readout
which is reasonable only for very short echo times.

Following their ideas,
we then derived a two-compartment dess signal model,
which interestingly required additional approximations
when including exchange
unlese we assumed that the difference
of compartmental off-resonance frequencies
remained constant over time,
which might be a mild assumption.

Even with all the highlighted assumptions,
closed-form signal models still remain elusive,

and so for simplicity,
we neglect exchange in the ensuing studies.
This may be a reasonable assumption
when describing the interaction
of myelin water
and other water,
as interactions across the hydrophobic myelin membrane
are rather slow (~200ms) compared
to myelin water fraction t2 (~15-40ms).


To estimate six free latent parameters,
at minimum six datasets are required,
and so the scan parameter optimization
for these six datasets is higher-dimensional,
making grid search to optimize the min-max cost less desirable.
Instead,
we use gradient information
to locally optimize the bayesian scan design cost function.

Here x is six-dimensional,
nu again assumes known flip angle variation,
and P again contains nominal flip angles
and repetition times.

Weighting matrix W is fixed
to only place emphasis
on the fast-fraction ff,
and using the inverse mean
rescales the cost function
to be interpretable
as the expected coefficient of variation
of unbiased estimates of ff in wm.

Expectations are approximated
via sample means of samples drawn
from a separable prior,

and the constraint space ensures
reasonable flip angles
and enforces a total scan time constraint.


Here are the optimized flip angles
and repetition times
of a scan profile
designed under a total time constraint
comparable to that of mcDESPOT.
We see that the predicted mwf relative std dev
is less than 30%,
which is a significant improvement
over similar calculations
performed for the mcDESPOT acquisition.


We first studied mwf estimation in simulation.
At each voxel position,
we simulated ground-truth x,nu
and used the optimized scan design
to generate single-voxel measurement vector y.

We then used PERK to estimate just the fast-fraction estimate,
and list here are the only parameters
that we had to choose manually,
with the exception of the Gaussian kernel shape.

We then compared PERK estimates against grid search estimates.
To limit computation,
we implemented the grid search
on a search space
that we artificially constrained
to extend only 20% beyond the ground truth.


Here are the fast-fraction estimates.
From left to right are the ground truth,
grid search estimate,
and the PERK estimate.
The estimates exhibit similar levels
of estimation accuracy and precision,

but PERK is more than 300 times faster.


Most recently,
we tried our proposed MWF protocol in vivo.
Over this matrix size,
the acquisition took about 12 minutes,
including time for separate flip angle calibration.

Now, full-scale grid search would be required for comparison
and at least in matlab 
would require months of computation time
on a typical desktop computer.

On the other hand,
PERK required about 35s for training
and 5s/slice for testing.

For qualitative comparison,
I'll also show mwf estimates
from a recent comparison study 
of GRASE, an accelerated MESE acquisition
and mcDESPOT.


Here are proof-of-concept in vivo MWF estimates.
The left two estimates are from the comparison study
while the righthand estimate 
is from scan optimization and PERK estimation.
The proposed MWF acquisition
is as fast as mcDESPOT
and produces MWF estimates
comparable to those of GRASE.


In summary,
we introduced a fast acquisition
for precise MWF estimation in WM
and so far have shown
what appear to be reasonable in vivo MWF estimates.

As ongoing work,
we will need to systematically validate
our acquisition and estimation 
through a comparison study
with MESE MWF estimates.

Longer term,
I am interested in applying scan design and PERK
to other QMRI applications
such as MR fingerprinting,
and we have some preliminary work
in this direction
in collaboration with Jakob.

I am also interested in correlating our MWF estimates
with other myelin biomarkers
such as a marker related 
to the inhomogeneous magnetization transfer effect.

Lastly,
I am interested in combining PERK
with image reconstruction
so as to better handle 
for example undersampled QMRI.


With that,
I'd like to acknowledge my collaborators
Jeff Fessler, Jon Nielsen, and Clay Scott
as well as our funding sources.
Thank you for your attention.

